
CHAPTER 12 EpidemiologyThe contributor for this chapter is David Madigan, professor and chair of statistics at Columbia. Madigan has over 100 publications in such areas as Bayesian statistics, text mining, Monte Carlo methods, phar‐ macovigilance, and probabilistic graphical models.Madigan’s BackgroundMadigan went to college at Trinity College Dublin in 1980, and spe‐ cialized in math except for his final year, when he took a bunch of stats courses, and learned a bunch about computers: Pascal, operating sys‐ tems, compilers, artificial intelligence, database theory, and rudimen‐ tary computing skills. He then worked in industry for six years, at both an insurance company and a software company, where he specialized in expert systems.It was a mainframe environment, and he wrote code to price insurance policies using what would now be described as scripting languages. He also learned about graphics by creating a graphic representation of a water treatment system. He learned about controlling graphics cards on PCs, but he still didn’t know about data.Next he got a PhD, also from Trinity College Dublin, and went into academia, and became a tenured professor at the University of Wash‐ ington. That’s when machine learning and data mining started, which he fell in love with: he was program chair of the KDD conference, among other things. He learned C and Java, R, and S+. But he still wasn’t really working with data yet.www.finebook.ir291
He claims he was still a typical academic statistician: he had computing skills but no idea how to work with a large-scale medical database, 50 different tables of data scattered across different databases with dif‐ ferent formats.In 2000 he worked for AT&T Labs. It was an “extreme academic en‐ vironment,” and he learned perl and did lots of stuff like web scraping. He also learned awk and basic Unix skills.He then went to an Internet startup where he and his team built a system to deliver real-time graphics on consumer activity.Since then he’s been working in big medical data stuff. He’s testified in trials related to medical trials (the word “trial” is used here in two different ways in this sentence), which was eye-opening for him in terms of explaining what you’ve done: “If you’re gonna explain logistic regression to a jury, it’s a different kind of a challenge than me standing here tonight.” He claims that super simple graphics help.Thought ExperimentWe now have detailed, longitudinal medical data on tens of millions of patients. What can we do with it?To be more precise, we have tons of phenomenological data: this is individual, patient-level medical record data. The largest of the data‐ bases has records on 80 million people: every prescription drug, every condition ever diagnosed, every hospital or doctor’s visit, every lab result, procedures, all timestamped.But we still do things like we did in the Middle Ages; the vast majority of diagnosis and treatment is done in a doctor’s brain. Can we do bet‐ ter? Can we harness these data to do a better job delivering medical care?This is a hugely important clinical problem, especially as a healthcare insurer. Can we intervene to avoid hospitalizations?So for example, there was a prize offered on Kaggle, called “Improve Healthcare, Win $3,000,000.” It challenged people to accurately predict who is going to go to the hospital next year. However, keep in mind that they’ve coarsened the data for privacy reasons.There are a lot of sticky ethical issues surrounding this 80 million person medical record dataset. What nefarious things could we do292 | Chapter 12: Epidemiologywww.finebook.ir
with this data? Instead of helping people stay well, we could use such models to gouge sick people with huge premiums, or we could drop sick people from insurance altogether.This is not a modeling question. It’s a question of what, as a society, we want to do with our models.Modern Academic StatisticsIt used to be the case, say 20 years ago, according to Madigan, that academic statisticians would either sit in their offices proving theo‐ rems with no data in sight—they wouldn’t even know how to run a t- test—or sit around in their offices and dream up a new test, or a new way of dealing with missing data, or something like that, and then they’d look around for a dataset to whack with their new method. In either case, the work of an academic statistician required no domain expertise.Nowadays things are different. The top stats journals are more deep in terms of application areas, the papers involve deep collaborations with people in social sciences or other applied sciences. Madigan sets an example by engaging with the medical community.Madigan went on to make a point about the modern machine learning community, which he is or was part of: it’s a newish academic field, with conferences and journals, etc., but from his perspective, it’s char‐ acterized by what statistics was 20 years ago: invent a method, try it on datasets. In terms of domain expertise engagement, it’s a step back‐ ward instead of forward.Not to say that statistics are perfect; very few academic statisticians have serious hacking skills, with Madigan’s colleague Mark Hansen being an unusual counterexample. In Madigan’s opinion, statisticians should not be allowed out of school unless they have such skills.Medical Literature and Observational StudiesAs you may not be surprised to hear, medical journals are full of ob‐ servational studies. The results of these studies have a profound effect on medical practice, on what doctors prescribe, and on what regulators do.For example, after reading the paper entitled “Oral bisphosphonates and risk of cancer of oesophagus, stomach, and colorectum: case-www.finebook.irModern Academic Statistics | 293
control analysis within a UK primary care cohort” (by Jane Green, et al.), Madigan concluded that we see the potential for the very same kind of confounding problem as in the earlier example with aspirin. The conclusion of the paper is that the risk of cancer increased with 10 or more prescriptions of oral bisphosphonates.It was published on the front page of the New York Times, the study was done by a group with no apparent conflict of interest, and the drugs are taken by millions of people. But the results might well be wrong and, indeed, were contradicted by later studies.There are thousands of examples of this. It’s a major problem and people don’t even get that it’s a problem.Billions upon billions of dollars are spent doing medical studies, and people’s lives depend on the results and the interpretations. We should really know if they work.Stratification Does Not Solve the Confounder ProblemThe field of epidemiology attempts to adjust for potential confound‐ ers. The bad news is that it doesn’t work very well. One reason is that the methods most commonly used rely heavily on stratification, which means partitioning the cases into subcases and looking at those. So, for example, if they think gender is a confounder, they’d adjust for gender in the estimator—a weighted average is one way of stratifying.But there’s a problem here, too. Stratification could make the under‐ lying estimates of the causal effects go from good to bad, especially when the experiment involves small numbers or when the populations are not truly similar.For example, say we have the situation shown in Table 12-1. Keep in mind that we cannot actually “see” the two counterfactual middle col‐ umns.Table 12-1. Aggregated: Both men and womenTreatment: DruggedTreatment: CounterfactualControl: CounterfactualControl: No DrugY=130203020Y=070807080P(Y=1)0.30.20.30.2294 | Chapter 12: Epidemiologywww.finebook.ir
Here we have 100 people in both the treatment and control groups, and in both the actual and counterfactual cases, we have a causal effect of 0.3 – 0.2 = 0.1, or 10%.But when we split this up by gender, we might introduce a problem, especially as the numbers get smaller, as seen in Tables 12-2 and 12-3.Table 12-2. Stratified: MenTable 12-3. Stratified: WomenOur causal estimate for men is 0.3 – 0.25 = 0.05, and for women is 0.3 – 0.1875 = 0.1125. A headline might proclaim that the drug has side effects twice as strong for women as for men.In other words, stratification doesn’t just solve problems. There are no guarantees your estimates will be better if you stratify. In fact, you should have very good evidence that stratification helps before you decide to do it.What Do People Do About Confounding Things in Practice?In spite of the raised objections, experts in this field essentially use stratification as a major method to working through studies. They deal with confounding variables, or rather variables they deem potentially confounding, by stratifying with respect to them or make other sorts of model-based adjustments, such as propensity score matching, for example. So if taking aspirin is believed to be a potentially confounding factor, they adjust or stratify with respect to it.Stratification Does Not Solve the Confounder Problem | 295Treatment: DruggedTreatment: CounterfactualControl: CounterfactualControl: No DrugY=115255Y=03586515P(Y=1)0.30.20.070.25Treatment: DruggedTreatment: CounterfactualControl: CounterfactualControl: No DrugY=115182515Y=03572565P(Y=1)0.30.20.830.1875www.finebook.ir
For example, with this study, which studied the risk of venous throm‐ boembolism from the use of certain kinds of oral contraceptives, the researchers chose certain confounders to worry about and concluded the following:After adjustment for length of use, users of oral contraceptives were at least twice the risk of clotting compared with users of other kinds of oral contraceptives.This report was featured on ABC, and was a big deal. But wouldn’t you worry about confounding issues like aspirin here? How do you choose which confounders to worry about? Or, wouldn’t you worry that the physicians who are prescribing them act different in different situa‐ tions, leading to different prescriptions? For example, might they give the newer one to people at higher risk of clotting?Another study came out about this same question and came to a dif‐ ferent conclusion, using different confounders. The researchers ad‐ justed for a history of clots, which makes sense when you think about it. Altogether we can view this as an illustration of how, depending on how one chooses to adjust for things, the outputs can vary wildly. It’s starting to seem like a hit or miss methodology.Another example is a study on oral bisphosphonates, where they ad‐ justed for smoking, alcohol, and BMI. How did they choose those variables? In fact, there are hundreds of examples where two teams made radically different choices on parallel studies.Madigan and some coauthors tested this by giving a bunch of epi‐ demiologists the job to design five studies at a high level. There was a low-level consistency. However, an additional problem is that lumi‐ naries of the field hear this and claim that they know the “right” way to choose the confounders.Is There a Better Way?Madigan and his coauthors examined 50 studies, each of which cor‐ responds to a drug and outcome pair (e.g., antibiotics with GI bleed‐ ing). They ran about 5,000 analyses for every pair—namely, every epistudy imaginable—and they did this all on nine different databases.For example, they fixed the drug to be ACE inhibitors and the outcome to be swelling of the heart. They ran the same analysis on the nine different standard databases, the smallest of which has records of296 | Chapter 12: Epidemiologywww.finebook.ir
4,000,000 patients, and the largest of which has records of 80,000,000 patients.In this one case, for one database, the drug triples the risk of heart swelling; but for another database, it seems to have a six-fold increase of risk. That’s one of the best examples, though, because at least it’s always bad news, which means it’s consistent.On the other hand, for 20 of the 50 pairs, you can go from statistically significant in one direction to the other direction depending on the database you pick. In other words, you can get whatever you want. Figure 12-1 shows a picture, where the heart swelling example is at the top.Figure 12-1. Self-controlled case seriesThe choice of database is rarely discussed in published epi‐ demiology papers.Next they did an even more extensive test, where they essentially tried everything. In other words, every time there was a decision to be made,www.finebook.irIs There a Better Way? | 297
they did it both ways. The kinds of decisions they tweaked were of the following types: which database you tested on, the confounders you accounted for, and the window of time you cared about examining, which refers to the situation where a patient has a heart attack a week or a month after discontinuing a treatment and whether that is counted in the study.What they saw was that almost all the studies can get either side de‐ pending on the choices.Let’s get back to oral bisphosphonates. A certain study concluded that they cause esophageal cancer, but two weeks later, JAMA published a paper on the same issue that concluded they are not associated with elevated risk of esophageal cancer. And they were even using the same database. This is not so surprising now for us.Research Experiment (Observational Medical Outcomes Partnership)To address the issues directly, or at least bring to light the limitations of current methods and results, Madigan has worked as a principal investigator on the OMOP research program, making significant contributions to the project’s methodological work including the de‐ velopment, implementation, and analysis of a variety of statistical methods applied to various observational databases.About OMOP, from Its WebsiteIn 2007, recognizing that the increased use of electronic health re‐ cords (EHR) and availability of other large sets of marketplace health data provided new learning opportunities, Congress directed the FDA to create a new drug surveillance program to more aggressively identify potential safety issues. The FDA launched several initiatives to achieve that goal, including the well-known Sentinel program to create a nationwide data network.In partnership with PhRMA and the FDA, the Foundation for the National Institutes of Health launched the Observational Medical Outcomes Partnership (OMOP), a public-private partnership. This interdisciplinary research group has tackled a surprisingly difficult task that is critical to the research community’s broader aims: identifying the most reliable methods for analyzing huge volumes of data drawn from heterogeneous sources.298 | Chapter 12: Epidemiologywww.finebook.ir
Employing a variety of approaches from the fields of epidemiology, statistics, computer science, and elsewhere, OMOP seeks to answer a critical challenge: what can medical researchers learn from assessing these new health databases, could a single approach be applied to multiple diseases, and could their findings be proven? Success would mean the opportunity for the medical research community to do more studies in less time, using fewer resources and achieving more consistent results. In the end, it would mean a better system for mon‐ itoring drugs, devices, and procedures so that the healthcare com‐ munity can reliably identify risks and opportunities to improve patient care.Madigan and his colleagues took 10 large medical databases, consist‐ ing of a mixture of claims from insurance companies and electronic health records (EHR), covering records of 200 million people in all. This is Big Data unless you talk to an astronomer.They mapped the data to a common data model and then they imple‐ mented every method used in observational studies in healthcare. Al‐ together they covered 14 commonly used epidemiology designs adap‐ ted for longitudinal data. They automated everything in sight. More‐ over, there were about 5,000 different “settings” on the 14 methods.The idea was to see how well the current methods do on predicting things we actually already know.To locate things they know, they took 10 old drug classes: ACE inhib‐ itors, beta blockers, warfarin, etc., and 10 outcomes of interest: renal failure, hospitalization, bleeding, etc.For some of these, the results are known. So, for example, warfarin is a blood thinner and definitely causes bleeding. There were nine such known bad effects.There were also 44 known “negative” cases, where we are super con‐ fident there’s just no harm in taking these drugs, at least for these outcomes.The basic experiment was this: run 5,000 commonly used epidemio‐ logical analyses using all 10 databases. How well do they do at dis‐ criminating between reds and blues? Kind of like a spam filter test, where one has training emails that are known spam, and one wants to know how well the model does at detecting spam when it comes through.Research Experiment (Observational Medical Outcomes Partnership) | 299www.finebook.ir
Each of the models output the same thing: a relative risk (RR) [meas‐ ured by the causal effect estimate we talked about previously] and an error.Theirs was an attempt to empirically evaluate how well epidemiology works, kind of the quantitative version of John Ioannidis’s work.Why Hasn’t This Been Done Before?There’s conflict of interest for epidemiology—why would they want to prove their methods don’t work? Also, it’s ex‐ pensive: it cost 25 million dollars, which of course pales in comparison to the money being put into these studies.They bought all the data, made the methods work automatically, and did a bunch of calculations in the Amazon cloud. The code is open source. In the second version, they zeroed in on four particular out‐ comes and built the $25,000,000 so-called ROC curve shown in Figure 12-2.Figure 12-2. The $25,000,000 ROC curveTo understand this graph, we need to define a threshold, which we canstart with at 2. This means that if the relative risk is estimated to be300 | Chapter 12: Epidemiologywww.finebook.ir
above 2, we call it a “bad effect”; otherwise we call it a “good effect.” The choice of threshold will, of course, matter.If it’s high, say 10, then you’ll never see a 10, so everything will be considered a good effect. Moreover, these are old drugs and wouldn’t be on the market. This means your sensitivity will be low, and you won’t find any real problem. That’s bad! You should find, for example, that warfarin causes bleeding.There’s of course good news too, with low sensitivity, namely a zero false-positive rate.What if you set the threshold really low, at –10? Then everything’s bad, and you have a 100% sensitivity but very high false-positive rate.As you vary the threshold from very low to very high, you sweep out a curve in terms of sensitivity and false-positive rate, and that’s the curve we see in the figure. There is a threshold (say, 1.8) for which your false positive rate is 30% and your sensitivity is 50%.This graph is seriously problematic if you’re the FDA. A 30% false-positive rate is not within the parameters that the FDA considers acceptable.The overall “goodness” of such a curve is usually measured as the area under the curve (AUC): you want it to be one, and if your curve lies on diagonal, the area is 0.5. This is tantamount to guessing randomly. So if your area under the curve is less than 0.5, it means your model is perverse.The AUC in the preceding figure is 0.64. Moreover, of the 5,000 anal‐ yses run by the research team (which included David Madigan), this is the single best analysis.But note: this is the best if you can only use the same method for ev‐ erything. In that case this is as good as it gets, and it’s not that much better than guessing.One the other hand, no epidemiologist would do that. So what they did next was to specialize the analysis to the database and the out‐ come. And they got better results: for the medicare database, and for acute kidney injury, their optimal model gives them an AUC of 0.92 as shown in Figure 12-3. They can achieve 80% sensitivity with a 10% false-positive rate.Research Experiment (Observational Medical Outcomes Partnership) | 301www.finebook.ir
Figure 12-3. Specializing the analysis to the database and the out‐ come with better resultsThey did this using a cross-validation method. Different databases have different methods attached to them. One winning method is called “OS,” which compares within a given patient’s history (so com‐ pares times when the patient was on drugs versus when they weren’t). This is not widely used now.The epidemiologists in general don’t believe the results of this study.If you go to http://elmo.omop.org, you can see the AUC for a given database and a given method. The data used in this study was current in mid-2010. To update this, you’d have to get the latest version of the database, and rerun the analysis. Things might have changed.Closing Thought ExperimentIn the study, 5,000 different analyses were run. Is there a good way of combining them to do better? How about incorporating weighted averages or voting methods across different strategies? The code is publicly available and might make a great PhD thesis.302 | Chapter 12: Epidemiologywww.finebook.ir