
CHAPTER 6 Time Stamps and Financial ModelingIn this chapter, we have two contributors, Kyle Teague from GetGlue, and someone you are a bit more familiar with by now: Cathy O’Neil. Before Cathy dives into her talk about the main topics for this chapter —times series, financial modeling, and fancypants regression—we’ll hear from Kyle Teague from GetGlue about how they think about building a recommendation system. (We’ll also hear more on this topic in Chapter 7.) We then lay some of the groundwork for thinking about timestamped data, which will segue into Cathy’s talk.Kyle Teague and GetGlueWe got to hear from Kyle Teague, a VP of data science and engineering at GetGlue. Kyle’s background is in electrical engineering. He consid‐ ers the time he spent doing signal processing in research labs as super valuable, and he’s been programming since he was a kid. He develops in Python.GetGlue is a New York-based startup whose primary goal is to address the problem of content discovery within the movie and TV space. The usual model for finding out what’s on TV is the 1950’s TV Guide schedule; that’s still how many of us find things to watch. Given that there are thousands of channels, it’s getting increasingly difficult to find out what’s good on TV.GetGlue wants to change this model, by giving people personalized TV recommendations and personalized guides. Specifically, userswww.finebook.ir135
“check in” to TV shows, which means they can tell other people they’re watching a show, thereby creating a timestamped data point. They can also perform other actions such as liking or commenting on the show.We store information in triplets of data of the form {user, action, item}, where the item is a TV show (or a movie). One way to visualize this stored data is by drawing a bipartite graph as shown in Figure 6-1.Figure 6-1. Bipartite graph with users and items (shows) as nodesWe’ll go into graphs in later chapters, but for now you should know that the dots are called “nodes” and the lines are called “edges.” This specific kind of graph, called a bipartite graph, is characterized by there being two kinds of nodes, in this case corresponding to “users” and “items.” All the edges go between a user and an item, specifically if the user in question has acted in some way on the show in question. There are never edges between different users or different shows. The graph136 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
in Figure 6-1 might display when certain users have “liked” certain TV shows.GetGlue enhances the graph as follows: it finds ways to create edges between users and between shows, albeit with different kinds of edges. So, for example, users can follow one another or be friends on GetGlue, which produces directed edges; i.e., edges going from one node to an‐ other with a direction, usually denoted by an arrow. Similarly, using the set of preferences, GetGlue can learn that two people have similar tastes and they can be connected that way as well, which would prob‐ ably not be directed.GetGlue also hires human evaluators to make connections or direc‐ tional edges between shows. So, for example, True Blood and Buffy the Vampire Slayer might be similar for some reason, and so the humans create a “similarity” edge in the graph between them. There are nu‐ ances around the edge being directional; they may draw an arrow pointing from Buffy to True Blood but not vice versa, for example, so their notion of “similar” or “close” captures both content and popu‐ larity. Pandora is purported to do something like this, too.Another important aspect, especially in the fast-paced TV space, is time. The user checked in or liked a show at a specific time, so the data that logs an action needs to have a timestamp as well: {user, action, item, timestamp}. Timestamps are helpful to see how influence pro‐ pogates in the graph we’ve constructed, or how the graph evolves over time.TimestampsTimestamped event data is a common data type in the age of Big Data. In fact, it’s one of the causes of Big Data. The fact that computers can record all the actions a user takes means that a single user can generate thousands of data points alone in a day. When people visit a website or use an app, or interact with computers and phones, their actions can be logged, and the exact time and nature of their interaction re‐ corded. When a new product or feature is built, engineers working on it write code to capture the events that occur as people navigate and use the product—that capturing is part of the product.For example, imagine a user visits the New York Times home page. The website captures which news stories are rendered for that user,www.finebook.irTimestamps | 137
and which stories were clicked on. This generates event logs. Each record is an event that took place between a user and the app or website.Here’s an example of raw data point from GetGlue:    {"userId": "rachelschutt", "numCheckins": "1",    "modelName": "movies", "title": "Collaborator",    "source": "http://getglue.com/stickers/tribeca_film/    collaborator_coming_soon", "numReplies": "0",    "app": "GetGlue", "lastCheckin": "true",    "timestamp": "2012-05-18T14:15:40Z",    "director": "martin donovan", "verb": "watching",    "key": "rachelschutt/2012-05-18T14:15:40Z",    "others": "97", "displayName": "Rachel Schutt",    "lastModified": "2012-05-18T14:15:43Z",    "objectKey": "movies/collaborator/martin_donovan",    "action": "watching"}If we extract four fields: {"userid":"rachelschutt", "action": "watching", "title":"Collaborator", timestamp:"2012-05- 18T14:15:40Z" }, we can think of it as being in the order we just discussed, namely {user, verb, object, timestamp}.Exploratory Data Analysis (EDA)As we described in Chapter 2, it’s best to start your analysis with EDA so you can gain intuition for the data before building models with it. Let’s delve deep into an example of EDA you can do with user data, stream-of-consciousness style. This is an illustration of a larger tech‐ nique, and things we do here can be modified to other types of data, but you also might need to do something else entirely depending on circumstances.The very first thing you should look into when dealing with user data is individual user plots over time. Make sure the data makes sense to you by investigating the narrative the data indicates from the perspec‐ tive of one person.To do this, take a random sample of users: start with something small like 100 users. Yes, maybe your dataset has millions of users, but to start out, you need to gain intuition. Looking at millions of data points is too much for you as a human. But just by looking at 100, you’ll start to understand the data, and see if it’s clean. Of course, this kind of sample size is not large enough if you were to start making inferences about the entire set of data.138 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
You might do this by finding usernames and grepping or searching for 100 random choices, one at a time. For each user, create a plot like the one in Figure 6-2.Figure 6-2. An example of a way to visually display user-level data over timeNow try to construct a narrative from that plot. For example, we could say that user 1 comes the same time each day, whereas user 2 started out active in this time period but then came less and less frequently. User 3 needs a longer time horizon for us to understand his or her behavior, whereas user 4 looks “normal,” whatever that means.Let’s pose questions from that narrative:• What is the typical or average user doing?• What does variation around that look like?• How would we classify users into different segments based on their behavior with respect to time?• How would we quantify the differences between these users?www.finebook.irTimestamps | 139
Think abstractly about a certain typical question from data munging discipline. Say we have some raw data where each data point is an event, but we want to have data stored in rows where each row consists of a user followed by a bunch of timestamps corresponding to actions that user performed. How would we get the data to that point? Note that different users will have a different number of timestamps.Make this reasoning explicit: how would we write the code to create a plot like the one just shown? How would we go about tackling the data munging exercise?Suppose a user can take multiple actions: “thumbs_up,” or “thumbs_down,” “like,” and “comment.” How can we plot those events? How can we modify our metrics? How can we encode the user data with these different actions? Figure 6-3 provides an example for the first question where we color code actions thumbs up and thumbs down, denoted thumbs_up and thumbs_down.Figure 6-3. Use color to include more information about user actions in a visual displayIn this toy example, we see that all the users did the same thing at the same time toward the right end of the plots. Wait, is this a real event or a bug in the system? How do we check that? Is there a large co- occurence of some action across users? Is “black” more common than “red”? Maybe some users like to always thumb things up, another140 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
group always like to thumb things down, and some third group of users are a mix. What’s the definition of a “mix”?Now that we’ve started to get some sense of variation across users, we can think about how we might want to aggregate users. We might make the x-axis refer to time, and the y-axis refer to counts, as shown in Figure 6-4.Figure 6-4. Aggregating user actions into countsWe’re no longer working with 100 individual users, but we’re still making choices, and those choices will impact our perception and understanding of the dataset.For example, are we counting the number of unique users or the overall number of user logins? Because some users log in multiple times, this can have a huge impact. Are we counting the number of actions or the number of users who did a given action at least once during the given time segment?What is our time horizon? Are we counting per second, minute, hour, 8-hour segments, day, or week? Why did we choose that? Is the signal overwhelmed by seasonality, or are we searching for seasonality?www.finebook.irTimestamps | 141
Are our users in different time zones? If user 1 and user 2 are in New York and London, respectively, then it’s 7 a.m. in NYC when it’s noon in London. If we count that as 7 a.m. and say “30,000 users did this action in the morning,” then it’s misleading, because it’s not morning in London. How are we going to treat this? We could shift the data into buckets so it’s 7 a.m. for the user, not 7 a.m. in New York, but then we encounter other problems. This is a decision we have to make and justify, and it could go either way depending on circumstances.Timestamps Are TrickyWe’re not gonna lie, timestamps are one of the hardest things to get right about modeling, especially around time changes. That’s why it’s sometimes easiest to convert all timestamps into seconds since the beginning of epoch time.Maybe we want to make different plots for different action types, or maybe we will bin actions into broader categories to take a good look at them and gain perspective.Metrics and New Variables or FeaturesThe intuition we gained from EDA can now help us construct metrics. For example, we can measure users by keeping tabs on the frequencies or counts of their actions, the time to first event, or simple binary variables such as “did action at least once,” and so on. If we want to compare users, we can construct similarity or difference metrics. We might want to aggregate by user by counting or having a cumulative sum of counts or money spent, or we might aggregate by action by having cumulative or average number of users doing that action once or more than once.This is by no means a comprehensive list. Metrics can be any function of the data, as long as we have a purpose and reason for creating them and interpreting them.What’s Next?We want to start moving toward modeling, algorithms, and analysis, incorporating the intuition we built from the EDA into our models and algorithms. Our next steps depend on the context, but here are some examples of what we could do.142 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
We might be interested in time series modeling, which includes auto- regression. We’ll talk about this more in the next section on financial modeling, but generally speaking we work with time series when we are trying to predict events that are super time-sensitive, like markets, or somewhat predictable based on what already happened, such as how much money gets invested in pension funds per month.We might start clustering, as we discussed in Chapter 3. In order to do this, we’d need to define the closeness of users with each other.Maybe we’ll want to build a monitor that could automatically detect common behavior patterns. Of course we’d first have to define what a common behavior pattern is and what would make things uncommon.We might try our hands at change-point detection, which is to say the ability to identify when some big event has taken place. What kind of behavior in our system should trigger an alarm? Or, we might try to establish causality. This can be very hard if we haven’t set up an ex‐ periment. Finally, we might want to train a recommendation system.Historical Perspective: What’s New About This?Timestamped data itself is not new, and time series analysis is a well- established field (see, for example, Time Series Analysis by James D. Hamilton). Historically, the available datasets were fairly small and events were recorded once a day, or even reported at aggregate levels. Some examples of timestamped datasets that have existed for a while even at a granular level are stock prices in finance, credit card trans‐ actions, phone call records, or books checked out of the library.Even so, there are a couple things that make this new, or at least the scale of it new. First, it’s now easy to measure human behavior throughout the day because many of us now carry around devices that can be and are used for measurement purposes and to record actions. Next, timestamps are accurate, so we’re not relying on the user to self- report, which is famously unreliable. Finally, computing power makes it possible to store large amounts of data and process it fairly quickly.Cathy O’NeilNext Cathy O’Neil spoke. You’re already familiar with her, but let’s check out her data science profile in Figure 6-5.www.finebook.irCathy O’Neil | 143
Figure 6-5. Cathy’s data science profileHer most obvious weakness is in CS. Although she programs in Python pretty proficiently, and can scrape and parse data, prototype models, and use matplotlib to draw pretty pictures, she is no Java map-reducer and bows down to those people who are. She’s also completely un‐ trained in data visualization, but knows enough to get by and give presentations that people understand.Thought ExperimentWhat do you lose when you think of your training set as a big pile of data and ignore the timestamps?The big point here is that you can’t tease out cause and effect if you don’t have any sense of time.What if we amend the question to allow the collection of relative time differentials, so “time since user last logged in” or “time since last click” or “time since last insulin injection,” but not absolute timestamps?In that case you still have major problems. For example, you’d ignore trends altogether, as well as seasonality, if you don’t order your data over time. So for the insulin example, you might note that 15 minutes after your insulin injection your blood sugar goes down consistently, but you might not notice an overall trend of your rising blood sugar over the past few months if your dataset for the past few months has no absolute timestamp on it. Without putting this data in order, you’d miss the pattern shown in Figure 6-6.144 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
Figure 6-6. Without keeping track of timestamps, we can’t see time- based patterns; here, we see a seasonal pattern in a time seriesThis idea, of keeping track of trends and seasonalities, is very impor‐ tant in financial data, and essential to keep track of if you want to make money, considering how small the signals are.Financial ModelingBefore the term data scientist existed, there were quants working in finance. There are many overlapping aspects of the job of the quant and the job of the data scientist, and of course some that are very different. For example, as we will see in this chapter, quants are sin‐ gularly obsessed with timestamps, and don’t care much about why things work, just if they do.Of course there’s a limit to what can be covered in just one chapter, but this is meant to give a taste of the kind of approach common in financial modeling.www.finebook.irFinancial Modeling | 145
In-Sample, Out-of-Sample, and CausalityWe need to establish a strict concept of in-sample and out-of-sample data. Note the out-of-sample data is not meant as testing data—that all happens inside in-sample data. Rather, out-of-sample data is meant to be the data you use after finalizing your model so that you have some idea how the model will perform in production.We should even restrict the number of times one does out-of-sample analysis on a given dataset because, like it or not, we learn stuff about that data every time, and we will subconsciously overfit to it even in different contexts, with different models.Next, we need to be careful to always perform causal modeling (note this differs from what statisticians mean by causality). Namely, never use information in the future to predict something now. Or, put differ‐ ently, we only use information from the past up and to the present moment to predict the future. This is incredibly important in financial modeling. Note it’s not enough to use data about the present if it isn’t actually available and accessible at the present moment. So this means we have to be very careful with timestamps of availability as well as timestamps of reference. This is huge when we’re talking about lagged government data.Similarly, when we have a set of training data, we don’t know the “best- fit coefficients” for that training data until after the last timestamp on all the data. As we move forward in time from the first timestamp to the last, we expect to get different sets of coefficients as more events happen.One consequence of this is that, instead of getting one set of “best-fit” coefficients, we actually get an evolution of each coefficient. This is helpful because it gives us a sense of how stable those coefficients are. In particular, if one coefficient has changed sign 10 times over the training set, then we might well expect a good estimate for it is zero, not the so-called “best fit” at the end of the data. Of course, depending on the variable, we might think of a legitimate reason for it to actually change sign over time.The in-sample data should, generally speaking, come before the out- of-sample data to avoid causality problems as shown in Figure 6-7.146 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
Figure 6-7. In-sample should come before out-of-sample data in a time series datasetOne final point on causal modeling and in-sample versus out-of- sample. It is consistent with production code, because we are always acting—in the training and in the out-of-sample simulation—as if we’re running our model in production and we’re seeing how it per‐ forms. Of course we fit our model in sample, so we expect it to perform better there than in production.Another way to say this is that, once we have a model in production, we will have to make decisions about the future based only on what we know now (that’s the definition of causal) and we will want to update our model whenever we gather new data. So our coefficients of our model are living organisms that continuously evolve. Just as they should—after all, we’re modeling reality, and things really change over time.Preparing Financial DataWe often “prepare” the data before putting it into a model. Typically the way we prepare it has to do with the mean or the variance of the data, or sometimes the data after some transformation like the log (andwww.finebook.irFinancial Modeling | 147
then the mean or the variance of that transformed data). This ends up being a submodel of our model.Transforming Your DataOutside of the context of financial data, preparing and transforming data is also a big part of the process. You have a number of possible techniques to choose from to transform your data to better “behave”:• Normalize the data by subtracting the mean and dividing by the standard deviation.• Alternatively normalize or scale by dividing by the maximum value.• Take the log of the data.• Bucket into five evenly spaced buckets; or five evenly distributed buckets (or a number other than five), and create a categorical variable from that.• Choose a meaningful threshold and transform the data into a new binary variable with value 1, if a data point is greater than or equal to the threshold, and 0 if less than the threshold.Once we have estimates of our mean y and variance σ2y, we can nor‐ malize the next data point with these estimates just like we do to get from a Gaussian or normal distribution to the standard normal dis‐ tribution with mean = 0 and standard deviation = 1:y y−y ↦ σyOf course we may have other things to keep track of as well to prepare our data, and we might run other submodels of our model. For ex‐ ample, we may choose to consider only the “new” part of something, which is equivalent to trying to predict something like yt − yt−1 instead of yt . Or, we may train a submodel to figure out what part of yt−1 predicts yt, such as a submodel that is a univariate regression or something.There are lots of choices here, which will always depend on the situa‐ tion and the goal you happen to have. Keep in mind, though, that it’s148 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
all causal, so you have to be careful when you train your overall model how to introduce your next data point and make sure the steps are all in order of time, and that you’re never ever cheating and looking ahead in time at data that hasn’t happened yet.In particular, and it happens all the time, one can’t normalize by the mean calculuated over the training set. Instead, have a running esti‐ mate of the mean, which you know at a given moment, and normalize with respect to that.To see why this is so dangerous, imagine a market crash in the middle of your training set. The mean and variance of your returns are heavily affected by such an event, and doing something as innocuous as a mean estimate translates into anticipating the crash before it happens. Such acausal interference tends to help the model, and could likely make a bad model look good (or, what is more likely, make a model that is pure noise look good).Log ReturnsIn finance, we consider returns on a daily basis. In other words, we care about how much the stock (or future, or index) changes from day to day. This might mean we measure movement from opening on Monday to opening on Tuesday, but the standard approach is to care about closing prices on subsequent trading days.We typically don’t consider percent returns, but rather log returns: if Ft denotes a close on day t, then the log return that day is defined as log Ft /Ft−1 , whereas the percent return would be computed as 100 Ft /Ft−1 −1 . To simplify the discussion, we’ll compare log re‐ turns to scaled percent returns, which is the same as percent returns except without the factor of 100. The reasoning is not changed by this difference in scalar.There are a few different reasons we use log returns instead of per‐ centage returns. For example, log returns are additive but scaled per‐ cent returns aren’t. In other words, the five-day log return is the sum of the five one-day log returns. This is often computationally handy.By the same token, log returns are symmetric with respect to gains and losses, whereas percent returns are biased in favor of gains. So, for example, if our stock goes down by 50%, or has a –0.5 scaled percent gain, and then goes up by 200%, so has a 2.0 scaled percent gain, we are where we started. But working in the same scenarios with logwww.finebook.irFinancial Modeling | 149
returns, we’d see first a log return of log 0.5 = −0.301 followed by a log return of log 2.0 = 0.301.Even so, the two kinds of returns are close to each other for smallish returns, so if we work with short time horizons, like daily or shorter, it doesn’t make a huge difference. This can be proven easily: setting x=Ft /Ft−1, the scaled percent return is x−1 and the log return is log x , which has the following Taylor expansion:logx=∑ x−1n=x−1+x−12/2+⋯ nnIn other words, the first term of the Taylor expansion agrees with the percent return. So as long as the second term is small compared to the first, which is usually true for daily returns, we get a pretty good ap‐ proximation of percent returns using log returns.Here’s a picture of how closely these two functions behave, keeping in mind that when x = 1, there’s no change in price whatsoever, as shown in Figure 6-8.Figure 6-8. Comparing log and scaled percent returns150 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
Example: The S&P IndexLet’s work out a toy example. If you start with S&P closing levels as shown in Figure 6-9, then you get the log returns illustrated in Figure 6-10.Figure 6-9. S&P closing levels shown over timeFigure 6-10. The log of the S&P returns shown over timewww.finebook.irFinancial Modeling | 151
What’s that mess? It’s crazy volatility caused by the financial crisis. We sometimes (not always) want to account for that volatility by normal‐ izing with respect to it (described earlier). Once we do that we get something like Figure 6-11, which is clearly better behaved.Figure 6-11. The volatility normalized log of the S&P closing returns shown over timeWorking out a Volatility MeasurementOnce we have our returns defined, we can keep a running estimate of how much we have seen it change recently, which is usually measured as a sample standard deviation, and is called a volatility estimate.A critical decision in measuring the volatility is in choosing a lookback window, which is a length of time in the past we will take our infor‐ mation from. The longer the lookback window is, the more informa‐ tion we have to go by for our estimate. However, the shorter our look‐ back window, the more quickly our volatility estimate responds to new information. Sometimes you can think about it like this: if a pretty big market event occurs, how long does it take for the market to “forget about it”? That’s pretty vague, but it can give one an intuition on the appropriate length of a lookback window. So, for example, it’s defi‐ nitely more than a week, sometimes less than four months. It also depends on how big the event is, of course.152 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
Next we need to decide how we are using the past few days’ worth of data. The simplest approach is to take a strictly rolling window, which means we weight each of the previous n days equally and a given day’s return is counted for those n days and then drops off the back of a window. The bad news about this easy approach is that a big return will be counted as big until that last moment, and it will completely disappear. This doesn’t jive with the ways people forget about things —they usually let information gradually fade from their memories.For this reason we instead have a continuous lookback window, where we exponentially downweight the older data and we have a concept of the “half-life” of the data. This works out to saying that we scale the impact of the past returns depending on how far back in the past they are, and for each day they get multiplied by some number less than 1 (called the decay). For example, if we take the number to be 0.97, then for five days ago we are multiplying the impact of that return by the scalar 0 . 975 . Then we will divide by the sum of the weights, and overall we are taking the weighted average of returns where the weights are just powers of something like 0.97. The “half-life” in this model can be inferred from the number 0.97 using these formulas as -ln(2)/ ln(0.97) = 23.Now that we have figured out how much we want to weight each pre‐ vious day’s return, we calculate the variance as simply the weighted sum of the squares of the previous returns. Then we take the square root at the end to estimate the volatility.Note we’ve just given you a formula that involves all of the previous returns. It’s potentially an infinite calculation, albeit with exponentially decaying weights. But there’s a cool trick: to actually compute this we only need to keep one running total of the sum so far, and combine it with the new squared return. So we can update our “vol” (as those in the know call volatility) estimate with one thing in memory and one easy weighted average.First, we are dividing by the sum of the weights, but the weights are powers of some number s, so it’s a geometric sum and in the limit, the sum is given by 1 / 1 − s .www.finebook.irFinancial Modeling | 153
Exponential DownweightingThis technique is called exponential downweighting, a con‐ venient way of compressing the data into a single value that can be updated without having to save the entire dataset.Next, assume we have the current variance estimate as:V o l d = 1 − s · ∑ i r 2i s iand we have a new return r0 to add to the series. Then it’s not hard toshow we just want:Vnew=s·Vold+ 1−s ·r20Note that we said we would use the sample standard deviation, but the formula for that normally involves removing the mean before taking the sum of squares. Here we ignore the mean, mostly because we are typically taking daily volatility, where the mean (which is hard to an‐ ticipate in any case!) is a much smaller factor than the noise, so we can treat it essentially as zero. If we were to measure volatility on a longer time scale such as quarters or years, then we would probably not ignore the mean.It really matters which downweighting factor you use, as shown in Figure 6-12.Indeed, we can game a measurement of risk (and people do) by choos‐ ing the downweighting factor that minimizes our risk.154 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
Figure 6-12. Volatility in the S&P with different decay factorsExponential DownweightingWe’ve already seen an example of exponential downweighting in the case of keeping a running estimate of the volatility of the returns of the S&P.The general formula for downweighting some additive running esti‐ mate E is simple enough. We weight recent data more than older data, and we assign the downweighting of older data a name s and treat it like a parameter. It is called the decay. In its simplest form we get:Et=s·Et−1+ 1−s ·et where et is the new term.www.finebook.irFinancial Modeling | 155
Additive EstimatesWe need each of our estimates to be additive (which is why we have a running variance estimate rather than a running standard deviation estimate). If what we’re after is a weighted average, say, then we will need to have a running estimate of both numerator and denominator.If we want to be really careful about smoothness at the beginning (which is more important if we have a few hundred data points or fewer), then we’ll actually vary the parameter s, via its reciprocal, which we can think of as a kind of half-life. We start with a half-life of 1 and grow it up to the asymptotic “true” half-life N = 1 / s. Thus, when we’re given a vector v of values et indexed by days t, we do something like this:    true_N = N    this_N_est = 1.0    this_E = 0.0    for e_t in v:        this_E = this_E * (1-1/this_N_est) + e_t * (1/this_N_est)        this_N_est = this_N_est*(1-1/true_N) + N * (1/true_N)The Financial Modeling Feedback LoopOne thing any quantitative person or data scientist needs to under‐ stand about financial modeling is that there’s a feedback loop. If you find a way to make money, it eventually goes away—sometimes people refer to this as the fact that the market “learns over time.”One way to see this is that, in the end, your model comes down to knowing some price (say) is going to go up in the future, so you buy it before it goes up, you wait, and then you sell it at a profit. But if you think about it, your buying it has actually changed the process, through your market impact, and decreased the signal you were anticipating. Of course, if you only buy one share in anticipation of the increase, your impact is minimal. But if your algorithm works really well, you tend to bet more and more, having a larger and larger impact. Indeed, why would you not bet more? You wouldn’t. After a while you’d learn the optimal amount you can bet and still make good money, and that optimal amount is large enough to have a big impact on the market.That’s how the market learns—it’s a combination of a bunch of algo‐ rithms anticipating things and making them go away.156 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
The consequence of this learning over time is that the existing signals are very weak. Things that were obvious (in hindsight) with the naked eye in the 1970s are no longer available, because they’re all understood and pre-anticipated by the market participants (although new ones might pop into existence).The bottom line is that, nowadays, we are happy with a 3% correlation for models that have a horizon of 1 day (a “horizon” for your model is how long you expect your prediction to be good). This means not much signal, and lots of noise! Even so, you can still make money if you have such an edge and if your trading costs are sufficiently small.In particular, lots of the machine learning “metrics of success” for models, such as measurements of precision or accuracy, are not very relevant in this context.So instead of measuring accuracy, we generally draw a picture to assess models as shown in Figure 6-13, namely of the (cumulative) PnL of the model. PnL stands for Profit and Loss and is the day-over-day change (difference, not ratio), or today’s value minus yesterday’s value.Figure 6-13. A graph of the cumulative PnLs of two theoretical modelswww.finebook.irFinancial Modeling | 157
This generalizes to any model as well—you plot the cumulative sum of the product of demeaned forecast and demeaned realized. (A de‐ meaned value is one where the mean’s been subtracted.) In other words, you see if your model consistently does better than the “stu‐ pidest” model of assuming everything is average.If you plot this and you drift up and to the right, you’re good. If it’s too jaggedy, that means your model is taking big bets and isn’t stable.Why Regression?So now we know that in financial modeling, the signal is weak. If you imagine there’s some complicated underlying relationship between your information and the thing you’re trying to predict, get over knowing what that is—there’s too much noise to find it. Instead, think of the function as possibly complicated, but continuous, and imagine you’ve written it out as a Taylor Series. Then you can’t possibly expect to get your hands on anything but the linear terms.Don’t think about using logistic regression, either, because you’d need to be ignoring size, which matters in finance—it matters if a stock went up 2% instead of 0.01%. But logistic regression forces you to have an on/off switch, which would be possible but would lose a lot of infor‐ mation. Considering the fact that we are always in a low-information environment, this is a bad idea.Note that although we’re claiming you probably want to use linear regression in a noisy environment, the actual terms themselves don’t have to be linear in the information you have. You can always take products of various terms as x’s in your regression. but you’re still fitting a linear model in nonlinear terms.Adding PriorsOne interpretation of priors is that they can be thought of as opinions that are mathematically formulated and incorporated into our models. In fact, we’ve already encountered a common prior in the form of downweighting old data. The prior can be described as “new data is more important than old data.”Besides that one, we may also decide to consider something like “co‐ efficients vary smoothly.” This is relevant when we decide, say, to use a bunch of old values of some time series to help predict the next one, giving us a model like:158 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
y=Ft =α0+α1Ft−1+α2Ft−2+εwhich is just the example where we take the last two values of the time series F to predict the next one. We could use more than two values, of course. If we used lots of lagged values, then we could strengthen our prior in order to make up for the fact that we’ve introduced so many degrees of freedom. In effect, priors reduce degrees of freedom.The way we’d place the prior about the relationship between coeffi‐ cients (in this case consecutive lagged data points) is by adding a matrix to our covariance matrix when we perform linear regression. See more about this here.A Baby ModelSay we drew a plot in a time series and found that we have strong but fading autocorrelation up to the first 40 lags or so as shown in Figure 6-14.Figure 6-14. Looking at auto-correlation out to 100 lagsWe can calculate autocorrelation when we have time series data. We create a second time series that is the same vector of data shifted by awww.finebook.irFinancial Modeling | 159
day (or some fixed time period), and then calculate the correlation between the two vectors.If we want to predict the next value, we’d want to use the signal that already exists just by knowing the last 40 values. On the other hand, we don’t want to do a linear regression with 40 coefficients because that would be way too many degrees of freedom. It’s a perfect place for a prior.A good way to think about priors is by adding a term to the function we are seeking to minimize, which measures the extent to which we have a good fit. This is called the “penalty function,” and when we have no prior at all, it’s simply the sum of the squares of the error:Fβ=∑i yi−xiβ2=y−xβτy−xβIf we want to minimize F, which we do, then we take its derivative with respect to the vector of coefficients β, set it equal to zero, and solve for β—there’s a unique solution, namely:β= xτx −1xτyIf we now add a standard prior in the form of a penalty term for largecoefficients, then we have:F1β=N1∑iyi−xiβ2+∑jλ2β2j=N1 y−xβτy−xβ+λIβτλIβ This can also be solved using calculus, and we solve for beta to get:β1= xτx+N·λ2I −1xτyIn other words, adding the penalty term for large coefficients translates into adding a scalar multiple of the identity matrix to the covariance matrix in the closed form solution to β.If we now want to add another penalty term that represents a “coeffi‐ cients vary smoothly” prior, we can think of this as requiring that adjacent coefficients should be not too different from each other, which can be expressed in the following penalty function with a new param‐ eter μ as follows:160 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir
F 2 β = N1 ∑ y i − x i β 2 + ∑ λ 2 β 2j + ∑ μ 2 β j − β j + 1 2 ijj= N1 y−xβ τ y−xβ +λ2βτβ+μ2 Iβ−Mβ τ Iβ−Mβwhere M is the matrix that contains zeros everywhere except on the lower off-diagonals, where it contains 1’s. Then Mβ is the vector that results from shifting the coefficients of β by one and replacing the last coefficient by 0. The matrix M is called a shift operator and the differ‐ ence I − M can be thought of as a discrete derivative operator (see here for more information on discrete calculus).Because this is the most complicated version, let’s look at this in detail. Remembering our vector calculus, the derivative of the scalar function F2 β with respect to the vector β is a vector, and satisfies a bunch of the properties that happen at the scalar level, including the fact that it’s both additive and linear and that:∂uτ · u = 2 ∂uτ u ∂β ∂βPutting the preceding rules to use, we have:∂F2β =1 ∂y−xβτy−xβ/ +λ2·∂βτβ+μ2·∂ I−Mβτ I−Mβ ∂βN∂β ∂β ∂β= −2 xτ y−xβ +2λ2·β+2μ2 I−M τ I−M β NSetting this to 0 and solving for β gives us: β2=xτx+N·λ2I+N·μ2·I−MτI−M −1xτyIn other words, we have yet another matrix added to our covariance matrix, which expresses our prior that coefficients vary smoothly. Notethat the symmetric matrix I−M τ I−M has 1’s along its sub- and super-diagonal, but also has 2’s along its diagonal. In other words, we need to adjust our λ as we adjust our μ because there is an interaction between these terms.www.finebook.irFinancial Modeling | 161
Priors and Higher DerivativesIf you want to, you can add a prior about the second derivative (or other higher derivatives) as well, by squaring the deriva‐ tive operator  I − M  (or taking higher powers of it).So what’s the model? Well, remember we will choose an exponential downweighting term γ for our data, and we will keep a running esti‐ mate of both xτ x and xτ y as was explained previously. The hyper‐ parameters of our model are then γ, λ, and μ. We usually have a sense of how large γ should be based on the market, and the other two pa‐ rameters depend on each other and on the data itself. This is where it becomes an art—you want to optimize to some extent on these choices, without going overboard and overfitting.Exercise: GetGlue and Timestamped Event DataGetGlue kindly provided a dataset for us to explore their data, which contains timestamped events of users checking in and rating TV shows and movies.The raw data is on a per-user basis, from 2007 to 2012, and only shows ratings and check-ins for TV shows and movies. It’s less than 3% of their total data, and even so it’s big, namely 11 GB once it’s uncom‐ pressed.Here’s some R code to look at the first 10 rows in R:## author: Jared Lander #require(rjson) require(plyr)    # the location of the data    dataPath <- "http://getglue-data.s3.amazonaws.com/                 getglue_sample.tar.gz"    # build a connection that can decompress the file    theCon <- gzcon(url(dataPath))    # read 10 lines of the data    n.rows <- 10    theLines <- readLines(theCon, n=n.rows)| Chapter 6: Time Stamps and Financial Modeling162www.finebook.ir
    # check its structurestr(theLines)# notice the first element is different than the rest theLines[1]    # use fromJSON on each element of the vector, except the firsttheRead <- lapply(theLines[-1], fromJSON) # turn it all into a data.frametheData <- ldply(theRead, as.data.frame) # see how we didView(theData) Start with these steps:1. Load in 1,000 rows of data and spend time looking at it with your eyes. For this entire assignment, work with these 1,000 rows until your code is in good shape. Then you can extend to 100,000 or 1 million rows.2. Aggregate and count data. Find answers to the following questions:• How many unique actions can a user take? And how many ac‐ tions of each type are in this dataset?• How many unique users are in this dataset?• What are the 10 most popular movies?• How many events in this dataset occurred in 2011?3. Propose five new questions that you think are interesting and worth investigating.4. Investigate/answer your questions.5. Visualize. Come up with one visualization that you think captures something interesting about this dataset.Exercise: Financial DataHere’s an exercise to help you explore the concepts in this chapter:1. Get the Data: Go to Yahoo! Finance and download daily data from a stock that has at least eight years of data, making sure it goes from earlier to later. If you don’t know how to do it, Google it.Exercise: GetGlue and Timestamped Event Data | 163www.finebook.ir
2. Create the time series of daily log returns of the stock price.3. Just for comparison, do the same for volume data (i.e., create the time series of daily log changes in volume).4. Next, try to set up a linear regression model that uses the past two returns to predict the next return. Run it and see if you can make any money with it. Try it for both stock returns and volumes. Bonus points if you: do a causal model, normalize for volatility (standard deviation), or put in an exponential decay for old data.5. Draw the cumulative P&L (forecast × realized) graphs and see if they drift up.164 | Chapter 6: Time Stamps and Financial Modelingwww.finebook.ir