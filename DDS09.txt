
CHAPTER 9 Data Visualization and Fraud DetectionThere are two contributors for this chapter, Mark Hansen, a professor at Columbia University, and Ian Wong, an inference scientist at Square. (That’s where he was in November 2012 when he came to the class. He now works at Prismatic.) These two speakers and sections don’t share a single cohesive theme between them, although both will discuss data visualization...and both have lived in California! More seriously, both are thoughtful people (like all our contributors) who have thought deeply about themes and questions such as what makes good code, the nature of programming languages as a form of expression, and the central question of this book: what is data science?Data Visualization HistoryFirst up is Mark Hansen, who recently came from UCLA via a sab‐ batical at the New York Times R & D Lab to Columbia University with a joint appointment in journalism and statistics, where he heads the Brown Institute for Media Innovation. He has a PhD in statistics from Berkeley, and worked at Bell Labs (there’s Bell Labs again!) for several years prior to his appointment at UCLA, where he held an appoint‐ ment in statistics, with courtesy appointments in electrical engineering and design/media art. He is a renowned data visualization expert and also an energetic and generous speaker. We were lucky to have him on a night where he’d been drinking an XXL latte from Starbucks (we refuse to use their made-up terminology) to highlight his natural ef‐ fervescence.www.finebook.ir217
Mark will walk us through a series of influences and provide historical context for his data visualization projects, which he will tell us more about at the end. Mark’s projects are genuine works of art—installa‐ tions appearing in museums and public spaces. Rachel invited him because his work and philosophy is inspiring and something to aspire to. He has set his own course, defined his own field, exploded bound‐ aries, and constantly challenges the status quo. He’s been doing data visualization since before data visualization was cool, or to put it another way, we consider him to be one of the fathers of data visuali‐ zation. For the practical purposes of becoming better at data visuali‐ zation yourself, we’ll give you some ideas and directions at the end of the chapter.Gabriel TardeMark started by telling us a bit about Gabriel Tarde, who was a soci‐ ologist who believed that the social sciences had the capacity to pro‐ duce vastly more data than the physical sciences.As Tarde saw it, the physical sciences observe from a distance: they typically model or incorporate models that talk about an aggregate in some way—for example, a biologist might talk about the function of the aggregate of our cells. What Tarde pointed out was that this is a deficiency; it’s basically brought on by a lack of information. According to Tarde, we should instead be tracking every cell.In the social realm we can do the analog of this, if we replace cells with people. We can collect a huge amount of information about individ‐ uals, especially if they offer it up themselves through Facebook.But wait, are we not missing the forest for the trees when we do this? In other words, if we focus on the microlevel, we might miss the larger cultural significance of social interaction. Bruno Latour, a contempo‐ rary French sociologist, weighs in with his take on Tarde in Tarde’s Idea of Quantification:But the whole is now nothing more than a provisional visualization which can be modified and reversed at will, by moving back to the individual components, and then looking for yet other tools to re‐ group the same elements into alternative assemblages.— Bruno LatourIn 1903, Tarde even foresaw the emergence of Facebook, as a sort of “daily press”:218 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
If statistics continues to progress as it has done for several years, if the information which it gives us continues to gain in accuracy, in dispatch, in bulk, and in regularity, a time may come when upon the accomplishment of every social event a figure will at once issue forth automatically, so to speak, to take its place on the statistical registers that will be continuously communicated to the public and spread abroad pictorially by the daily press. Then, at every step, at every glance cast upon poster or newspaper, we shall be assailed, as it were, with statistical facts, with precise and condensed knowledge of all the peculiarities of actual social conditions, of commercial gains or losses, of the rise or falling off of certain political parties, of the progress or decay of a certain doctrine, etc., in exactly the same way as we are assailed when we open our eyes by the vibrations of the ether which tell us of the approach or withdrawal of such and such a so-called body and of many other things of a similar nature.— Tarde Mark then laid down the theme of his lecture:Change the instruments and you will change the entire social theory that goes with them.— Bruno LatourKind of like that famous physics cat, Mark (and Tarde) want us to newly consider both the way the structure of society changes as we observe it, and ways of thinking about the relationship of the individ‐ ual to the aggregate.In other words, the past nature of data collection methods forced one to consider aggregate statistics that one can reasonably estimate by subsample—means, for example. But now that one can actually get one’s hands on all data and work with all data, one no longer should focus only on the kinds of statistics that make sense in the aggregate, but also one’s own individually designed statistics—say, coming from graph-like interactions—that are now possible due to finer control. Don’t let the dogma that resulted from past restrictions guide your thinking when those restrictions no longer hold.Mark’s Thought ExperimentAs data become more personal, as we collect more data about indi‐ viduals, what new methods or tools do we need to express the funda‐ mental relationship between ourselves and our communities, our communities and our country, our country and the world?www.finebook.irData Visualization History | 219
Could we ever be satisfied with poll results or presidential approval ratings when we can see the complete trajectory of public opinions, both individuated and interacting?To which we add: would we actually want to live in a culture where such information is so finely tracked and available?What Is Data Science, Redux?Mark reexamined the question that Rachel posed and attempted to answer in the first chapter, as he is keen on reexamining everything. He started the conversation with this undated quote from our own John Tukey:The best thing about being a statistician is that you get to play in everyone’s backyard.— John TukeyLet’s think about that again—is it so great? Is it even reasonable? In some sense, to think of us as playing in other people’s yards, with their toys, is to draw a line between “traditional data fields” and “everything else.”It’s maybe even implying that all our magic comes from the traditional data fields (math, stats, CS), and we’re some kind of super humans because we’re uber-nerds. That’s a convenient way to look at it from the perspective of our egos, of course, but it’s perhaps too narrow and arrogant.And it begs the question: what is “traditional” and what is “everything else,” anyway?In Mark’s opinion, “everything else” should include fields from social science and physical science to education, design, journalism, and media art. There’s more to our practice than being technologists, and we need to realize that technology itself emerges out of the natural needs of a discipline. For example, geographic information systems (GIS) emerged from geographers, and text data mining emerged from digital humanities.In other words, it’s not math people ruling the world, but rather do‐ main practices being informed by techniques growing organically from those fields. When data intersects their practice, each practice is learning differently; their concerns are unique to that practice.220 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Responsible data science integrates those lessons, and it’s not a purely mathematical integration. It could be a way of describing events, for example. Specifically, we’re saying that it’s not necessarily a quantifia‐ ble thing.Bottom-line: it’s possible that the language of data science has some‐ thing to do with social science just as it has something to do with math.You might not be surprised to hear that, when Mark told us about his profile as a data scientist, the term he coined was “expansionist.”ProcessingMark then described the programming language called Processing in the context of a programming class he gave to artists and designers. He used it as an example of what is different when a designer, versus an engineer, takes up looking at data or starts to code. A good language is inherently structured or designed to be expressive of the desired tasks and ways of thinking of the people using it.One approach to understanding this difference is by way of another thought experiment. Namely, what is the use case for a language for artists? Contrast this with what a language such as R needs to capture for statisticians or data scientists (randomness, distributions, vectors, and data, for example).In a language for artists, you’d want to be able to specify shapes, to faithfully render whatever visual thing you had in mind, to sketch, possibly in 3D, to animate, to interact, and most importantly, to publish.Processing is Java-based, with a simple “publish” button, for example. The language is adapted to the practice of artists. Mark mentioned that teaching designers to code meant, for him, stepping back and talking about iteration, if statements, and so on—in other words, stuff that seemed obvious to him but is not obvious to someone who is an artist. He needed to unpack his assumptions, which is what’s fun about teaching to the uninitiated.Franco MorettiMark moved on to discussing close versus distant reading of texts. He mentioned Franco Moretti, a literary scholar from Stanford.What Is Data Science, Redux? | 221www.finebook.ir
Franco thinks about “distant reading,” which means trying to get a sense of what someone’s talking about without reading line by line. This leads to PCA-esque thinking, a kind of dimension reduction of novels (recall we studied dimension reduction techniques in Chap‐ ter 8).Mark holds this up as a cool example of how ideally, data science in‐ tegrates the ways that experts in various fields already figure stuff out. In other words, we don’t just go into their backyards and play; maybe instead we go in and watch them play, and then formalize and inform their process with our own bells and whistles. In this way they can teach us new games, games that actually expand our fundamental conceptions of data and the approaches we need to analyze them.A Sample of Data Visualization ProjectsHere are some of Mark’s favorite visualization projects, and for each one he asks us: is this your idea of data visualization? What’s data?Figure 9-1 is a projection onto a power plant’s steam cloud. The size of the green projection corresponds to the amount of energy the city is using.Figure 9-1. Nuage Vert by Helen Evans and Heiko Hansen 222 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
In One Tree (Figure 9-2) the artist cloned trees and planted the ge‐ netically identical seeds in several areas. It displays, among other things, the environmental conditions in each area where they are planted.Figure 9-2. One Tree by Natalie Jeremijenkowww.finebook.irA Sample of Data Visualization Projects | 223
Figure 9-3 shows Dusty Relief, in which the building collects pollution around it, displayed as dust.Figure 9-3. Dusty Relief from New Territories224 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Project Reveal (Figure 9-4) is a kind of magic mirror that wirelessly connects using facial recognition technology and gives you informa‐ tion about yourself. According to Mark, as you stand at the mirror in the morning, you get that “come-to-Jesus moment.”Figure 9-4. Project Reveal from the New York Times R & D labThe SIDL is headed by Laura Kurgan, and in this piece shown in Figure 9-5, she flipped Google’s crime statistics. She went into the prison population data, and for every incarcerated person, she looked at their home address, measuring per home how much money the state was spending to keep the people who lived there in prison. She dis‐ covered that some blocks were spending $1,000,000 to keep people in prison. The moral of this project is: just because you can put something on the map, doesn’t mean you should. It doesn’t mean there’s a new story. Sometimes you need to dig deeper and flip it over to get a new story.www.finebook.irA Sample of Data Visualization Projects | 225
Figure 9-5. Million Dollar Blocks from Spatial Information Design Lab (SIDL)Mark’s Data Visualization ProjectsNow that we know some of Mark’s influences and philosophy, let’s look at some of his projects to see how he puts them into practice.New York Times Lobby: Moveable TypeMark walked us through a project he did with Ben Rubin—a media artist and Mark’s collaborator of many years—for the New York Times on commission. (Mark later went to the New York Times R & D Lab on sabbatical.) Figure 9-6 shows it installed in the lobby of the Times’ midtown Manhattan headquarters at 8th Avenue and 42nd Street.226 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Figure 9-6. Moveable Type, the New York Times lobby, by Ben Rubin and Mark HansenIt consists of 560 text displays—two walls with 280 displays on each— and they cycle through various scenes that each have a theme and an underlying data science model.In one there are waves upon waves of digital ticker-tape–like scenes that leave behind clusters of text, and where each cluster represents a different story from the paper. The text for a given story highlights phrases that make a given story different from others in an information-theory sense.In another scene, the numbers coming out of stories are highlighted, so you might see “18 gorillas” on a given display. In a third scene, crossword puzzles play themselves accompanied by sounds of pencils writing on paper.Figure 9-7 shows an example of a display box, which are designed to convey a retro vibe. Each box has an embedded Linux processor run‐ ning Python, and a sound card that makes various sounds—clicking, typing, waves—depending on what scene is playing.Mark’s Data Visualization Projects | 227www.finebook.ir
Figure 9-7. Display box for Moveable TypeThe data is collected via text from New York Times articles, blogs, and search engine activity. Every sentence is parsed using Stanford natural language processing techniques, which diagram sentences.228 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Altogether there are about 15 scenes so far, and it’s written in code so one can keep adding to it. Here’s a YouTube interview with Mark and Ben about the exhibit.Project Cascade: Lives on a ScreenMark next told us about Cascade, which was a joint work with Jer Thorp—data artist-in-residence at the New York Times —in partner‐ ship with bit.ly. Cascade came about from thinking about how people share New York Times links on Twitter.The idea was to collect enough data so that you could see people browse, encode the link in bitly, tweet that encoded link, see other people click on that tweet, watch bitly decode the link, and then see those people browse the New York Times. Figure 9-8 shows the visu‐ alization of that entire process, much like Tarde suggested we should do.Figure 9-8. Project Cascade by Jer Thorp and Mark HansenThere were of course data decisions to be made: a loose matching of tweets and clicks through time, for example. If 17 different tweets provided the same URL, they couldn’t know which tweet/link some‐ one clicked on, so they guessed (the guess actually involves probabil‐Mark’s Data Visualization Projects | 229www.finebook.ir
istic matching on timestamps so at least it’s an educated guess). They used the Twitter map of who follows who—if someone you follow tweets about something before you do, then it counts as a retweet.Here’s a video about Project Cascade from the New York Times.This was done two years ago, and Twitter has gotten a lotbigger since then.Cronkite PlazaNext Mark told us about something he was working on with both Jer and Ben. It’s also news-related, but it entailed projecting something on the outside of a building rather than in the lobby; specifically, the communications building at UT Austin, in Cronkite Plaza, pictured in Figure 9-9.Figure 9-9. And That’s The Way It Is, by Jer Thorp, Mark Hansen, and Ben Rubin230 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
It’s visible every evening at Cronkite Plaza, with scenes projected onto the building via six different projectors. The majority of the projected text is sourced from Walter Cronkite’s news broadcasts, but they also used local closed-captioned news sources. One scene extracted the questions asked during local news—things like “How did she react?” or “What type of dog would you get?”eBay Transactions and BooksAgain working jointly with Jer Thorp, Mark investigated a day’s worth of eBay’s transactions that went through Paypal and, for whatever rea‐ son, two years of book sales. How do you visualize this? Take a look at their data art-visualization-installation commissioned by eBay for the 2012 ZERO1 Biennial in Figure 9-10 and at the yummy underlying data in Figure 9-11.Figure 9-10. Before Us is the Salesman’s House (2012), by Jer Thorp and Mark Hansenwww.finebook.irMark’s Data Visualization Projects | 231
Figure 9-11. The underlying data for the eBay installationHere’s their ingenious approach: They started with the text of Death of a Salesman by Arthur Miller. They used a mechanical turk mecha‐ nism (we discussed what this means in Chapter 7) to locate objects in the text that you can buy on eBay.When an object is found it moves it to a special bin, e.g., “chair” or “flute” or “table.” When it has a few collected buyable objects, it then takes the objects and sees where they are all for sale on the day’s worth of transactions, and looks at details on outliers and such. After exam‐ ining the sales, the code will find a zip code in some quiet place like Montana.232 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Then it flips over to the book sales data, looks at all the books bought or sold in that zip code, picks a book (which is also on Project Guten‐ berg), and begins to read that book and collect “buyable” objects from it. And it keeps going. Here’s a video showing the process.Public Theater Shakespeare MachineThe last piece Mark showed us is a joint work with Rubin and Thorp, installed in the lobby of the Public Theater, shown in Figure 9-12. The piece is an oval structure with 37 bladed LED displays installed above the theater’s bar.There’s one blade for each of Shakespeare’s plays. Longer plays are in the long end of the oval—you see Hamlet when you come in.Figure 9-12. Shakespeare Machine, by Mark, Jer, and BenThe data input is the text of each play. Each scene does something different—for example, it might collect noun phrases that have some‐ thing to do with the body from each play, so the “Hamlet” blade will only show a body phrase from Hamlet. In another scene, various kinds of combinations or linguistic constructs are mined, such as three-wordMark’s Data Visualization Projects | 233www.finebook.ir
phrases like “high and might” or “good and gracious” or compound- word phrases like “devilish-holy,” “heart-sore,” or “hard-favoured.”Note here that the digital humanities, through the MONK Project, offered intense XML descriptions of the plays. Every single word is given hooha, and there’s something on the order of 150 different parts of speech.As Mark said, it’s Shakespeare, so it stays awesome no matter what you do. But they’re also successively considering words as symbols, or as thematic, or as parts of speech.So then let’s revisit the question Mark asked before showing us all these visualizations: what’s data? It’s all data.Here’s one last piece of advice from Mark on how one acquires data. Be a good investigator: a small polite voice which asks for data usually gets it.Goals of These ExhibitsThese exhibits are meant to be graceful and artistic, but they should also teach something or tell a story. At the same time, we don’t want to be overly didactic. The aim is to exist in between art and informa‐ tion. It’s a funny place: increasingly we see a flattening effect when tools are digitized and made available, so that statisticians can code like a designer—we can make things that look like design, but is it truly design—and similarly designers can make something that looks like data or statistics, but is it really?Data Science and RiskNext we had a visitor from San Francisco—Ian Wong, who came to tell us about doing data science on the topic of risk. Ian is an inference scientist at Square, and he previously dropped out of the electrical engineering PhD program at Stanford where he did research in ma‐ chine learning. (He picked up a couple master’s degrees in statistics and electrical engineering along the way.) Since coming to speak to the class, he left Square and now works at Prismatic, a customized newsfeed.Ian started with three takeaways:234 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Doing machine learning != writing R scriptsMachine learning (ML) is founded in math, expressed in code, and assembled into software. You need to develop good software engineering practices, and learn to write readable and reusable code. Write code for the reader and not the writer, as production code is reread and built upon many more times by other people than by you.Data visualization != producing a nice plotVisualizations should be pervasive and embedded in the environ‐ ment of a good company. They’re integrated in products and pro‐ cesses. They should enable action.ML and data visualization together augment human intelligenceWe have limited cognitive abilities as human beings. But by lev‐ eraging data, we can build ourselves exoskeletons that enable us to comprehend and navigate the information world.About SquareSquare was founded in 2009 by Jack Dorsey and Jim McKelvey. The company grew from 50 employees in 2011 to over 500 in 2013.The mission of the company is to make commerce easy for everyone. As Square’s founders see it, transactions are needlessly complicated. It takes too much for a merchant to figure out how to accept payments. For that matter, it’s too complicated for buyers to pay as well. The question they set out to answer is “how do we make transactions simple and easy?”Here’s how they do it. Merchants can sign up with Square, download the Square Register app, and receive a credit card reader in the mail. They can then plug the reader into the phone, open the app, and take payments. The little plastic square enables small merchants (any size really) to acept credit card transactions. Local hipster coffee shops seem to have been early adopters if Portland and San Francisco are any indication. On the consumer’s side, they don’t have to do anything special, just hand over their credit cards. They won’t experience any‐ thing unusual, although they do sign on the iPad rather than on a slip of paper.It’s even possible to buy things hands-free using the Square. When the buyer chooses to pay through Square Wallet on their phones, thewww.finebook.irData Science and Risk | 235
buyer’s name will appear on the merchant’s Register app and all the merchant has to do is to tap on the name.Square wants to make it easy for sellers to sign up for their service and to accept payments. Of course, it’s also possible that somebody may sign up and try to abuse the service. They are, therefore, very careful at Square to avoid losing money on sellers with fraudulent intentions or bad business models.The Risk ChallengeIn building a frictionless experience for buyers and sellers, Square also has to watch out for the subset of users who abuse the service. Suspi‐ cious or unwanted activity, such as fraud, not only undermines cus‐ tomer trust, but is illegal and impacts the company’s bottom line. So creating a robust and highly efficient risk management system is core to the payment company’s growth.But how does Square detect bad behavior efficiently? Ian explained that they do this by investing in machine learning with a healthy dose of visualization.Detecting suspicious activity using machine learningLet’s start by asking: what’s suspicious? If we see lots of micro trans‐ actions occurring, say, or if we see a sudden, high frequency of trans‐ actions, or an inconsistent frequency of transactions, that might raise our eyebrows.Here’s an example. Say John has a food truck, and a few weeks after he opens, he starts to pass $1,000 transactions through Square. (One possibility: John might be the kind of idiot that puts gold leaf on ham‐ burgers.) On the one hand, if we let money go through, Square is on the spot in case it’s a bad charge. Technically the fraudster—who in this case is probably John—would be liable, but our experience is that usually fraudsters are insolvent, so it ends up on Square to foot the bill.On the other hand, if Square stops payment on what turns out to be a real payment, it’s bad customer service. After all, what if John is inno‐ cent and Square denies the charge? He will probably be pissed at Square —and he may even try to publicly sully Square’s reputation—but in any case, the trust is lost with him after that.This example crystallizes the important challenges Square faces: false positives erode customer trust, false negatives cost Square money.236 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
To be clear, there are actually two kinds of fraud to worry about: seller- side fraud and buyer-side fraud. For the purpose of this discussion, we’ll focus on seller-side fraud, as was the case in the story with John.Because Square processes millions of dollars worth of sales per day, they need to gauge the plausibility of charges systematically and au‐ tomatically. They need to assess the risk level of every event and entity in the system.So what do they do? Before diving in, Ian sketched out part of their data schema, shown in Figures 9-13, 9-14, and 9-15.Figure 9-13. Payment schemaFigure 9-14. Seller schemaFigure 9-15. Settlement schemawww.finebook.irData Science and Risk| 237
There are three types of data represented here:• Payment data, where we can assume the fields are transaction_id, seller_id, buyer_id, amount, success (0 or 1), and timestamp.• Seller data, where we can assume the fields are seller_id, sign_up_date, business_name, business_type, and business_lo‐ cation.• Settlement data, where we can assume the fields are settlement_id, state, and timestamp.It’s important to note that Square settles with its customers a full day after the initial transaction, so their process doesn’t have to make a decision within microseconds. They’d like to do it quickly of course, but in certain cases, there is time for a phone call to check on things.Here’s the process shown in Figure 9-16: given a bunch (as in millions) of payment events and their associated date (as shown in the data schema earlier), they throw each through the risk models, and then send some iffy-looking ones on to a “manual review.” An ops team will then review the cases on an individual basis. Specifically, anything that looks rejectable gets sent to ops, who follow up with the merchants. All approved transactions are settled (yielding an entry in the settle ment table).Figure 9-16. Risk engineGiven the preceding process, let’s focus on how they set up the risk models. You can think of the model as a function from payments to labels (e.g., good or bad). Putting it that way, it kind of sounds like a straightforward supervised learning problem. And although this problem shares some properties with that, it’s certainly not that simple —they don’t reject a payment and then merely stand pat with that label,238 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
because, as we discussed, they send it on to an ops team to assess it independently. So in actuality they have a pretty complicated set of labels, including when a charge is initially rejected but later decide it’s OK, or it’s initially accepted but on further consideration might have been bad, or it’s confirmed to have been bad, or confirmed to have been OK, and the list goes on.Technically we would call this a semi-supervised learning problem, straddling the worlds of supervised and unsupervised learning. But it’s useful to note that the “label churn” settles down after a few months when the vast majority of chargebacks have been received, so they could treat the problem as strictly supervised learning if you go far enough back in time. So while they can’t trust the labels on recent data, for the purpose of this discussion, Ian will describe the easier case of solving the supervised part.Now that we’ve set the stage for the problem, Ian moved on to de‐ scribing the supervised learning recipe as typically taught in school:• Get data.• Derive features.• Train model.• Estimate performance.• Publish model!But transferring this recipe to the real-world setting is not so simple. In fact, it’s not even clear that the order is correct. Ian advocates think‐ ing about the objective first and foremost, which means bringing per‐ formance estimation to the top of the list.The Trouble with Performance EstimationSo let’s do that: focus on performance estimation. Right away Ian identifies three areas where we can run into trouble.Defining the error metricHow do we measure whether our learning problem is being modeled well? Let’s remind ourselves of the various possibilities using the truth table in Table 9-1.www.finebook.irData Science and Risk | 239
Table 9-1. Actual versus predicted table, also called the Confusion MatrixActual = True Predicted = True TP (true positive) Predicted = False FP (false positive)Actual = FalseFP (false positive) FN (false negative)The most straightforward performance metric is Accuracy, which is defined using the preceding notation as the ratio:Accuracy= TP+TN TP+TN+FP+FNAnother way of thinking about accuracy is that it’s the probability that your model gets the right answer. Given that there are very few positive examples of fraud—at least compared with the overall number of transactions—accuracy is not a good metric of success, because the “everything looks good” model, or equivalently the “nothing looks fraudulent” model, is dumb but has good accuracy.Instead, we can estimate performance using Precision and Recall. Pre‐ cision is defined as the ratio:Precision = TP TP+FPor the probability that a transaction branded “fraudulent” is actually fraudulent.Recall is defined as the ratio:Recall= TP TP+FNor the probability that a truly fraudulent transaction is caught by the model.The decision of which of these metrics to optimize for depends on the costs of uncaught bad transactions, which are easy to measure, versus overly zealous caught transactions, which are much harder to measure.240 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Defining the labelsLabels are what Ian considered to be the “neglected” half of the data. In undergrad statistics education and in data mining competitions, the availability of labels is often taken for granted. But in reality, labels are tough to define and capture, while at the same time they are vitally important. It’s not related to just the objective function; it is the objective.In Square’s setting, defining the label means being precise about:• What counts as a suspicious activity?• What is the right level of granularity? An event or an entity (or both)?• Can we capture the label reliably? What other systems do we need to integrate with to get this data?Lastly, Ian briefly mentioned that label noise can acutely affect pre‐ diction problems with high class imbalance (e.g., very few positive samples).Challenges in features and learningIan says that features codify your domain knowledge. Once a machine learning pipeline is up and running, most of the modeling energy should be spent trying to figure out better ways to describe the domain (i.e., coming up with new features). But you have to be aware of when these features can actually be learned.More precisely, when you are faced with a class imbalanced problem, you have to be careful about overfitting. The sample size required to learn a feature is proportional to the population of interest (which, in this case, is the “fraud” class).For example, it can get tricky dealing with categorical variables with many levels. While you may have a zip code for every seller, you don’t have enough information in knowing the zip code alone because so few fraudulant sellers share zip codes. In this case, you want to do some clever binning of the zip codes. In some cases, Ian and his team create a submodel within a model just to reduce the dimension of certain features.There’s a second data sparsity issue, which is the cold start problem with new sellers. You don’t know the same information for all of yourwww.finebook.irData Science and Risk | 241
sellers, especially for new sellers. But if you are too conservative, you risk starting off on the wrong foot with new customers.Finally, and this is typical for predictive algorithms, you need to tweak your algorithm to fit the problem setting, which in this case is akin to finding a needle in a haystack. For example, you need to consider whether features interact linearly or nonlinearly, and how to adjust model training to account for class imbalance: should you adjust the weights for each class? How about the sampling scheme in an ensemble learner?You also have to be aware of adversarial behavior, which is a way of saying that someone is actually scheming against you. Here’s an ex‐ ample from ecommerce: if a malicious buyer figures out that you are doing fraud detection by address resolution via exact string matching, then he can simply sign up with 10 new accounts, each with a slight misspelling of same address. You now need to know how to resolve these variants of addresses, and anticipate the escalation of adverserial behavior. Because models degrade over time as people learn to game them, you need to continually track performance and retrain your models.What’s the Label?Here’s another example of the trickiness of labels. At DataEDGE, a conference held annually at UC Berkeley’s School of Information, in a conversation between Michael Chui of the McKinsey Global Insti‐ tute, and Itamar Rosenn—Facebook’s first data scientist (hired in 2007)—Itamar described the difficulties in defining an “engaged user.” What’s the definition of engaged? If you want to predict whether or not a user is engaged, then you need some notion of engaged if you’re going to label users as engaged or not. There is no one obvious defi‐ nition, and, in fact, a multitude of definitions might work depending on the context—there is no ground truth! Some definitions of en‐ gagement could depend on the frequency or rhythm with which a user comes to a site, or how much they create or consume content. It’s a semi-supervised learning problem where you’re simultaneously trying to define the labels as well as predict them.242 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Model Building TipsHere are a few good guidelines to building good production models:Models are not black boxesYou can’t build a good model by assuming that the algorithm will take care of everything. For instance, you need to know why you are misclassifying certain people, so you’ll need to roll up your sleeves and dig into your model to look at what happened. You essentially form a narrative around the mistakes.Develop the ability to perform rapid model iterationsThink of this like experiments you’d do in a science lab. If you’re not sure whether to try A or B, then try both. Of course there’s a limit to this, but most of the time people err on the “not doing it enough” side of things.Models and packages are not a magic potionWhen you hear someone say, “So which models or packages do you use?” then you’ve run into someone who doesn’t get it.Ian notices that the starting point of a lot of machine learning discus‐ sions revolves around what algorithm or package people use. For in‐ stance, if you’re in R, people get caught up on whether you’re using randomForst, gbm, glmnet, caret, ggplot2, or rocr; or in scikit- learn (Python), whether you’re using the RandomForestClassifier or RidgeClassifier. But that’s losing sight of the forest.Code readability and reusabilitySo if it’s not about the models, what is it really about then? It’s about your ability to use and reuse these packages, to be able to swap any and all of these models with ease. Ian encourages people to concentrate on readability, reusability, correctness, structure, and hygiene of these code bases.And if you ever dig deep and implement an algorithm yourself, try your best to produce understandable and extendable code. If you’re coding a random forest algorithm and you’ve hardcoded the number of trees, you’re backing yourself (and anyone else who uses that algo‐ rithm) into a corner. Put a parameter there so people can reuse it. Make it tweakable. Favor composition. And write tests, for pity’s sake. Clean code and clarity of thought go together.www.finebook.irData Science and Risk | 243
At Square they try to maintain reusability and readability by struc‐ turing code in different folders with distinct, reusable components that provide semantics around the different parts of building a machine learning model:ModelThe learning algorithmsSignalData ingestion and feature computationErrorPerformance estimationExperimentScripts for exploratory data analysis and experimentsTestTest all the thingsThey only write scripts in the experiments folder where they either tie together components from model, signal, and error, or conduct ex‐ ploratory data analysis. Each time they write a script, it’s more than just a piece of code waiting to rot. It’s an experiment that is revisited over and over again to generate insight.What does such a discipline give you? Every time you run an experi‐ ment, you should incrementally increase your knowledge. If that’s not happening, the experiment is not useful. This discipline helps you make sure you don’t do the same work again. Without it you can’t even figure out the things you or someone else has already attempted. Ian further claims that “If you don’t write production code, then you’re not productive.”For more on what every project directory should contain, see Project Template by John Myles White. For those students who are using R for their classes, Ian suggests exploring and actively reading Github’s repository of R code. He says to try writing your own R package, and make sure to read Hadley Wickham’s devtools wiki. Also, he says that developing an aesthetic sense for code is analogous to acquiring the taste for beautiful proofs; it’s done through rigorous practice and feed‐ back from peers and mentors.For extra credit, Ian suggests that you contrast the implementations of the caret package with scikit-learn. Which one is more extendable and reusable? Why?244 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Get a pair!Learning how to code well is challenging. And it’s even harder to go about it alone. Imagine trying to learn Spanish (or your favorite lan‐ guage) and not being able to practice with another person.Find a partner or a team who will be willing to pair program and conduct rigorous code reviews. At Square, every single piece of code is reviewed by at least another pair of eyes. This is an important prac‐ tice not only for error checking purposes, but also to ensure shared ownership of the code and a high standard of code quality.Here’s something to try once you find a programming buddy. Identify a common problem. Set up a workstation with one monitor and two sets of keyboard and mouse. Think of it as teaming up to solve the problem: you’ll first discuss the overall strategy of solving the problem, and then move on to actually implementing the solution. The two of you will take turns being the “driver” or the “observer.” The driver writes the code, while the observer reviews the code and strategizes the plan of attack. While the driver is busy typing, the observer should constantly be asking “do I understand this code?” and “how can this code be clearer?” When confusion arises, take the time to figure out the misunderstandings (or even lack of understanding) together. Be open to learn and to teach. You’ll pick up nuggets of knowledge quick‐ ly, from editor shortcuts to coherent code organization.The driver and observer roles should switch periodically throughout the day. If done right, you’ll feel exhausted after several hours. Practice to improve pairing endurance.And when you don’t get to pair program, develop the habit to check in code using git. Learn about git workflows, and give each other constructive critiques on pull requests. Think of it as peer review in academia.Productionizing machine learning modelsHere are some of the toughest problems in doing real-world machine learning:1. How is a model “productionized”?2. How are features computed in real time to support these models?www.finebook.irData Science and Risk | 245
3. How do we make sure that “what we see is what we get”? That is, minimizing the discrepency between offline and online perfor‐ mance.In most classes and in ML competitions, predictive models are pitted against a static set of hold-out data that fits in memory, and the models are allowed to take as long as they need to run. Under these lax con‐ straints, modelers are happy to take the data, do an O(n^3) operation to get features, and run it through the model. Complex feature engi‐ neering is often celebrated for pedagogical reasons. Examples:• High-dimensionality? Don’t worry, we’ll just do an SVD, save off the transformation matrices, and multiply them with the hold-out data.• Transforming arrival rate data? Hold on, let me first fit a Poisson model across the historical data and hold-out set.• Time series? Let’s throw in some Fourier coefficients.Unfortunately, real life strips away this affordance of time and space. Predictive models are pitted against an ever-growing set of online data. In many cases, they are expected to yield predictions within millisec‐ onds after being handed a datum. All that hard work put into building the model won’t matter unless the model can handle the traffic.Keep in mind that many models boil down to a dot product of features with weights (GLM, SVM), or a series of conjuctive statements with thresholds that can be expressed as array lookups or a series of if-else statements against the features (decision trees). So the hard part re‐ duces to feature computation.There are various approaches to computing features. Depending on model complexity, latency, and volume requirements, features are computed in either batch or real time. Models may choose to consume just batch features, just real-time features, or both. In some cases, a real-time model makes use of real-time features plus the output of models trained in batch.Frameworks such as MapReduce are often used to compute features in batch. But with a more stringent latency requirement, Ian and the machine learning team at Square are working on a real-time feature computation system.246 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
An important design goal of such a system is to ensure that historical and online features are computed in the same way. In other words, there should not be systematic differences between the online features and the historical features. Modelers should have confidence that the online performance of the models should match the expected performance.Data Visualization at SquareNext Ian talked about ways in which the Risk team at Square uses visualization:• Enable efficient transaction review.• Reveal patterns for individual customers and across customer segments.• Measure business health.• Provide ambient analytics.He described a workflow tool to review users, which shows features of the seller, including the history of sales and geographical information, reviews, contact info, and more. Think mission control. This workflow tool is a type of data visualization. The operation team tasked with reviewing suspicious activity has limited time to do their work, so developers and data scientists must collaborate with the ops team to figure out ways to best represent the data. What they’re trying to do with these visualizations is to augment the intelligence of the opera‐ tions team, to build them an exoskeleton for identifying patterns of suspicious activity. Ian believes that this is where a lot of interesting development will emerge—the seamless combination of machine learning and data visualization.Visualizations across customer segments are often displayed in various TVs in the office (affectionately known as “information radiators” in the Square community). These visualizations aren’t necessarily trying to predict fraud per se, but rather provide a way of keeping an eye on things to look for trends and patterns over time.This relates to the concept of “ambient analytics,” which is to provide an environment for constant and passive ingestion of data so you can develop a visceral feel for it. After all, it is via the process of becoming very familiar with our data that we sometimes learn what kind of pat‐ terns are unusual or which signals deserve their own model orwww.finebook.irData Visualization at Square | 247
monitor. The Square risk team has put in a lot of effort to develop custom and generalizable dashboards.In addition to the raw transactions, there are risk metrics that Ian keeps a close eye on. So, for example, he monitors the “clear rates” and “freeze rates” per day, as well as how many events needed to be reviewed. Using his fancy viz system he can get down to which analysts froze the most today, how long each account took to review, and what attributes in‐ dicate a long review process.In general, people at Square are big believers in visualizing business metrics—sign-ups, activations, active users—in dashboards. They be‐ lieve the transparency leads to more accountability and engagement. They run a kind of constant EKG of their business as part of ambient analytics. The risk team, in particular, are strong believers in “What gets measured gets managed.”Ian ended with his data scientist profile and a few words of advice. He thinks plot(skill_level ~ attributes | ian) should be shown via a log-y scale, because it doesn’t take very long to be OK at something but it takes lots of time to get from good to great. He believes that productivity should also be measured in log-scale, and his argument is that leading software contributors crank out packages at a much higher rate than other people.And as parting advice, Ian encourages you to:• Play with real data.• Build math, stats, and computer science foundations in school.• Get an internship.• Be literate, not just in statistics.• Stay curious!Ian’s Thought ExperimentSuppose you know about every single transaction in the world as it occurs. How would you use that data?248 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Data Visualization for the Rest of UsNot all of us can create data visualizations considered to be works of art worthy of museums, but it’s worth building up one’s ability to use data visualization to communicate and tell stories, and convey the meaning embedded in the data. Just as data science is more than a set of tools, so is data visualization, but in order to become a master, one must first be able to master the technique. Following are some tutorials and books that we have found useful in building up our data visuali‐ zation skills:• There’s a nice orientation to the building blocks of data visuali‐ zation by Michael Dubokov at http://www.targetprocess.com/arti cles/visual-encoding.html.• Nathan Yau, who was Mark Hansen’s PhD student at UCLA, has a collection of tutorials on creating visualizations in R at http:// flowingdata.com/. Nathan Yau also has two books: Visualize This: The Flowing Data Guide to Design, Visualization, and Statistics (Wiley); and Data Points: Visualization That Means Something (Wiley).• Scott Murray, code artist, has a series of tutorials to get up to speed on d3 at http://alignedleft.com/tutorials/d3/. These have been de‐ veloped into a book, Interactive Data Visualization (O’Reilly).• Hadley Wickham, who developed the R package ggplot2 based on Wilkinson’s Grammar of Graphics, has a corresponding book: ggplot2: Elegant Graphics for Data Analysis (Use R!) (Springer).• Classic books on data visualization include several books (The Visual Display of Quantitative Information [Graphics Pr], for ex‐ ample) by Edward Tufte (a statistician widely regarded as one of the fathers of data visualization; we know we already said that about Mark Hansen—they’re different generations) with an em‐ phasis less on the tools, and more on the principles of good design. Also William Cleveland (who we mentioned back in Chapter 1 because of his proposal to expand the field of statistics into data science), has two books: Elements of Graphing Data (Hobart Press) and Visualizing Data (Hobart Press).• Newer books by O’Reilly include the R Graphics Cookbook, Beau‐ tiful Data, and Beautiful Visualization.Data Visualization for the Rest of Us | 249www.finebook.ir
• We’d be remiss not to mention that art schools have graphic design departments and books devoted to design principles. An educa‐ tion in data visualization that doesn’t take these into account, as well as the principles of journalism and storytelling, and only fo‐ cuses on tools and statistics is only giving you half the picture. Not to mention the psychology of human perception.• This talk, “Describing Dynamic Visualizations” by Bret Victor comes highly recommended by Jeff Heer, a Stanford professor who created d3 with Michael Bostock (who used to work at Square, and now works for the New York Times). Jeff described this talk as presenting an alternative view of data visualization.• Collaborate with an artist or graphic designer!Data Visualization ExerciseThe students in the course, like you readers, had a wide variety of backgrounds and levels with respect to data visualization, so Rachel suggested those who felt like beginners go pick out two of Nathan Yau’s tutorials and do them, and then reflect on whether it helped or not and what they wanted to do next to improve their visualization skills.More advanced students in the class were given the option to partic‐ ipate in the Hubway Data Visualization challenge. Hubway is Boston’s bike-sharing program, and they released a dataset and held a compe‐ tition to visualize it. The dataset is still available, so why not give it a try? Two students in Rachel’s class, Eurry Kim and Kaz Sakamoto, won “best data narrative” in the competition; Rachel is very proud of them. Viewed through the lenses of a romantic relationship, their visual diary (shown in Figure 9-17) displays an inventory of their Boston residents’ first 500,000 trips together.250 | Chapter 9: Data Visualization and Fraud Detectionwww.finebook.ir
Figure 9-17. This is a visualization by Eurry Kim and Kaz Sakamoto of the Hubway shared-bike program and its adoption by the fine peo‐ ple of the Boston metro areawww.finebook.irData Visualization for the Rest of Us | 251
www.finebook.ir