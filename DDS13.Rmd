---
title: "DDS13"
author: "Robert A. Stevens"
date: "June 5, 2016"
output: html_document
---

```{r, comment=NA}

```

*Doing Data Science*

by Rachel Schutt and Cathy O’Neil

# CHAPTER 13 Lessons Learned from Data Competitions: Data Leakage and Model Evaluation

The contributor for this chapter is Claudia Perlich. Claudia has been the Chief Scientist at Media 6 Degrees (M6D) [1] for the past few years. Before that she was in the data analytics group at the IBM center that developed Watson [2], the computer that won Jeopardy! [3] (although she didn’t work on that project). Claudia holds a master’s in computer science, and got her PhD in information systems at NYU. She now teaches a class to business students on data science, where she addresses how to assess data science work and how to manage data scientists.

Claudia is also a famously successful data mining competition winner. She won the KDD Cup [4] in 2003, 2007, 2008, and 2009, the ILP Challenge [5] in 2005, the INFORMS Challenge [6] in 2008, and the Kaggle [7]  HIV competition in 2010.

More recently she’s turned toward being a data mining competition organizer, first for the INFORMS Challenge in 2009, and then for the Heritage Health Prize in 2011. Claudia claims to be retired from competition. Fortunately for the class, she provided some great insights into what can be learned from data competitions. From the many competitions she’s done, she’s learned quite a bit in particular about data leakage, and how to evaluate the models she comes up with for the competitions.

## Claudia’s Data Scientist Profile

Claudia started by asking what people’s reference point might be to evaluate where they stand with their own data science profile (hers is shown in Table 13-1. Referring to the data scientist profile from Chapter 1, she said, “There is one skill that you do not have here and that is the most important and the hardest to describe: Data.” She knows some of the world’s best mathematicians, machine language experts, statisticians, etc. Does she calibrate herself toward what is possible (the experts) or just relative to the average person in her field, or just an average person?

Table 13-1. Claudia’s data science profile
Skill            passable  strong solid comment
---------------- --------- ------ ----- ------------------------------------------------
Visualization    x         _      _     I can do it but I do not believe in visualization
Computer Science x         _      _     I have 2 Masters Degrees in CS. I can hack, not production code.
Math             x         _      _     That was a long time ago
Stats            _         x      _     Little formal training, a lot of stuff picked up on the way and good intuition
Machine Learning _         _      x     _
Domain           _         _      _     You are asking the wrong question...
Presentation     _         _      x     _
Data             _         _      x     _

### The Life of a Chief Data Scientist

Historically, Claudia has spent her time on predictive modeling, including data mining competitions, writing papers for publications and conferences like KDD and journals, giving talks, writing patents, teaching, and digging around data (her favorite part). She likes to understand something about the world by looking directly at the data.

Claudia’s skill set includes 15 years working with data, where she’s developed data intuition by delving into the data generating process, a crucial piece of the puzzle. She’s given a lot of time and thought to the evaluation process and developing model intuition.

Claudia’s primary skills are data manipulation using tools like Unix, sed, awk, Perl, and SQL. She models using various methods, including logistic regression, k-nearest neighbors, and, importantly, she spends a bunch of time setting things up well. She spends about 40% of her time as “contributor,” which means doing stuff directly with data; 40% of her time as “ambassador,” which means writing stuff, and giving talks, mostly external communication to represent M6D; and 20% of her time in “leadership” of her data group.

### On Being a Female Data Scientist

Being a woman works well in the field of data science, where intuition is useful and is regularly applied. One’s nose gets so well developed by now that one can smell it when something is wrong, although this is not the same thing as being able to prove something algorithmically. Also, people typically remember women, even when women don’t remember them. It has worked in her favor, Claudia says, which she’s happy to admit. But then again, she is where she is fundamentally because she’s good.

Being in academia, Claudia has quite a bit of experience with the process of publishing her work in journals and the like. She discussed whether papers submitted for journals and/or conferences are blind to gender. For some time, it was typically double-blind, but now it’s more likely to be one-sided. Moreover, there was a 2003 paper written by Shawndra Hill and Foster Provost [8] that showed you can guess who wrote a paper with 40% accuracy just by knowing the citations, and even more if the author had more publications. Hopefully people don’t actually use such models when they referee, but in any case, that means making things “blind” doesn’t necessarily help. More recently the names are included, and hopefully this doesn’t make things too biased. Claudia admits to being slightly biased herself toward institutions — in her experience, certain institutions prepare better work.

## Data Mining Competitions

Claudia drew a distinction between different types of data mining competitions. The first is the “sterile” kind, where you’re given a clean, prepared data matrix, a standard error measure, and features that are often anonymized. This is a pure machine learning problem.

Examples of this first kind are KDD Cup 2009 and the Netflix Prize, and many of the Kaggle competitions. In such competitions, your approach would emphasize algorithms and computation. The winner would probably have heavy machines and huge modeling ensembles.

KDD Cups

All the KDD Cups, with their tasks and corresponding datasets, can be found at http://www.kdd.org/kddcup/index.php. Here’s a list:

- KDD Cup 2010: Student performance evaluation

- KDD Cup 2009: Customer relationship prediction

- KDD Cup 2008: Breast cancer

- KDD Cup 2007: Consumer recommendations

- KDD Cup 2006: Pulmonary embolisms detection from image data

- KDD Cup 2005: Internet user search query categorization

- KDD Cup 2004: Particle physics; plus protein homology prediction

- KDD Cup 2003: Network mining and usage log analysis

- KDD Cup 2002: BioMed document; plus gene role classification

- KDD Cup 2001: Molecular bioactivity; plus protein locale prediction

- KDD Cup 2000: Online retailer website clickstream analysis

- KDD Cup 1999: Computer network intrusion detection

- KDD Cup 1998: Direct marketing for profit optimization

- KDD Cup 1997: Direct marketing for lift curve optimization

On the other hand, you have the “real world” kind of data mining competition, where you’re handed raw data (which is often in lots of different tables and not easily joined), you set up the model yourself, and come up with task-specific evaluations. This kind of competition simulates real life more closely, which goes back to Rachel’s thought experiment earlier in this book about how to simulate the chaotic experience of being a data scientist in the classroom. You need practice dealing with messiness.

Examples of this second kind are KDD cup 2007, 2008, and 2010. If you’re in this kind of competition, your approach would involve understanding the domain, analyzing the data, and building the model. The winner might be the person who best understands how to tailor the model to the actual question.

Claudia prefers the second kind, because it’s closer to what you do in real life.

## How to Be a Good Modeler

Claudia claims that data and domain understanding are the single most important skills you need as a data scientist. At the same time, this can’t really be taught — it can only be cultivated.

A few lessons learned about data mining competitions that Claudia thinks are overlooked in academics:

Leakage

The contestants’ best friend and the organizer and practitioners’ worst nightmare. There’s always something wrong with the data, and Claudia has made an artform of figuring out how the people preparing the competition got lazy or sloppy with the data.

Real-life performance measures

Adapting learning beyond standard modeling evaluation measures like mean squared error (MSE), misclassification rate, or area under the curve (AUC). For example, profit would be an example of a real-life performance measure.

Feature construction/transformation

Real data is rarely flat (i.e., given to you in a beautiful matrix) and good, practical solutions for this problem remain a challenge.

## Data Leakage

In a KDD 2011 paper that Claudia coauthored called “Leakage in Data Mining: Formulation, Detection, and Avoidance”, she, Shachar Kaufman, and Saharon Rosset point to another author, Dorian Pyle, who has written numerous articles and papers on data preparation in data mining, where he refers to a phenomenon that he calls anachronisms (something that is out of place in time), and says that “too good to be true” performance is “a dead giveaway” of its existence. Claudia and her coauthors call this phenomenon “data leakage” in the context of predictive modeling. Pyle suggests turning to exploratory data analysis in order to find and eliminate leakage sources. Claudia and her coauthors sought a rigorous methodology to deal with leakage.

Leakage refers to information or data that helps you predict something, and the fact that you are using this information to predict isn’t fair. It’s a huge problem in modeling, and not just for competitions. Oftentimes it’s an artifact of reversing cause and effect. Let’s walk through a few examples to get a feel for how this might happen.

### Market Predictions

There was a competition where you needed to predict S&P in terms of whether it would go up or go down. The winning entry had an AUC (area under the ROC curve) of 0.999 out of 1. Because stock markets are pretty close to random, either someone’s very rich or there’s something wrong. (Hint: there’s something wrong.)

In the good old days you could win competitions this way, by finding the leakage. It’s not clear in this case what the leakage was, and you’d only know if you started digging into the data and finding some piece of information in the dataset that was highly predictive of S & P, but that would not be available to you in real time when predicting S & P. We bring this example up because the very fact that they had such a high AUC means that their model must have been relying on leakage, and it would not work if implemented.

### Amazon Case Study: Big Spenders

The target of this competition was to predict customers who will be likely to spend a lot of money by using historical purchase data. The data consisted of transaction data in different categories. But a winning model identified that “Free Shipping = True” was an excellent predictor of spending a lot. Now notice, you only get offered free shipping after you’ve spent a certain amount of money, say above $50.

What happened here? The point is that free shipping is an effect of big spending. But it’s not a good way to model big spending, because in particular, it doesn’t work for new customers or for the future. Note: timestamps are weak here. The data that included “Free Shipping = True” was simultaneous with the sale, which is a no-no. You need to only use data from beforehand to predict the future. The difficulty is that this information about free shipping appeared in the collected data, and so it has to be manually removed, which requires consideration and understanding the data on the part of the model builder. If you weren’t thinking about leakage, you could just throw the free shipping variable into your model and see it predicted well. But, then when you actually went to implement the model in production, you wouldn’t know that the person was about to get free shipping.

### A Jewelry Sampling Problem

Again for an online retailer, this time the target was predicting customers who buy jewelry. The data consisted of transactions for different categories. A very successful model simply noted that if sum(revenue) = 0, then it predicted jewelry customers very well.

What happened here? The people preparing the data for the competition removed jewelry purchases, but only included people who bought something in the first place. So people who had sum(revenue) = 0 were people who only bought jewelry. The fact that only people who bought something got into the dataset is weird: in particular, you wouldn’t be able to use this on customers before they finished their purchase. So the model wasn’t being trained on the right data to make the model useful. This is a sampling problem, and it’s common.

Warning: Sampling Users

As mentioned, in this case it’s weird to only condition the analysis on the set of people who have already bought something. Do you really want to condition your analysis on only people who bought something or all people who came to your site? More generally with user-level data, if you’re not careful, you can make fairly simple, but serious sampling mistakes if you don’t think it through. For example, say you’re planning to analyze a dataset from one day of user traffic on your site. If you do this, you are oversampling users who come frequently.

Think of it this way with a toy example: suppose you have 80 users. Say that 10 of them come every day, and the others only come once a week. Suppose they’re spread out evenly over 7 days of the week. So on any given day you see 20 users. So you pick a day. You look at those 20 users — 10 of them are the ones who come every day, the other 10 come once a week. What’s happening is that you’re oversampling the users who come every day. Their behavior on your site might be totally different than other users, and they’re representing 50% of your dataset even though they only represent 12.5% of your user base.

### IBM Customer Targeting

At IBM, the target was to predict companies that would be willing to buy “websphere” solutions. The data was transaction data and crawled potential company websites. The winning model showed that if the term “websphere” appeared on the company’s website, then it was a great candidate for the product. What happened? Remember, when considering a potential customer, by definition that company wouldn’t have bought websphere yet (otherwise IBM wouldn’t be trying to sell to it); therefore no potential customer would have websphere on its site, so it’s not a predictor at all. If IBM could go back in time to see a snapshot of the historical Web as a source of data before the “websphere” solution product existed, then that data would make sense as a predictor. But using today’s data unfortunately contains the leaked information that they ended up buying websphere. You can’t crawl the historical Web, just today’s Web.

Seem like a silly, obvious mistake not to make? Maybe. But it’s the sort of thing that happens, and you can’t anticipate this kind of thing until you start digging into the data and really understand what features and predictors mean. Just think, if this happened with something “obvious,” it means that more careful thought and digging needs to go on to figure out the less obvious cases. Also, it’s an example of something maybe we haven’t emphasized enough yet in the book. Doing simple sanity checking to make sure things are what you think they are can sometimes get you much further in the end than web scraping and a big fancy machine learning algorithm. It may not seem cool and sexy, but it’s smart and good practice. People might not invite you to a meet-up to talk about it. It may not be publishable research, but at least it’s legitimate and solid work. (Though then again, because of this stuff, Claudia won tons of contests and gets invited to meetups all the time. So we take that back. No, we don’t. The point is, do good work, the rest will follow. Meetups and fame aren’t goals unto themselves. The pursuit of the truth is.)

### Breast Cancer Detection

You’re trying to study who has breast cancer. Take a look at Figure 13-1. The patient ID, which seems innocent, actually has predictive power. What happened?

In Figure 13-1, red means cancerous, green means not; it’s plotted by patient ID. We see three or four distinct buckets of patient identifiers. It’s very predictive depending on the bucket. This is probably a consequence of using multiple databases corresponding to different cancer centers, some of which take on sicker patients — by definition patients who get assigned to that center are more likely to be sick.

Figure 13-1. Patients ordered by patient identifier; red means cancerous, green means not

This situation led to an interesting discussion in the classroom:

Student: For the purposes of the contest, they should have renumbered the patients and randomized.

Claudia: Would that solve the problem? There could be other things in common as well.

Another student: The important issue could be to see the extent to which we can figure out which dataset a given patient came from based on things besides their ID.

Claudia: Think about this: what do we want these models for in the first place? How well can you really predict cancer?

Given a new patient, what would you do? If the new patient is in a fifth bin in terms of patient ID, then you wouldn’t want to use the identifier model. But if it’s still in this scheme, then maybe that really is the best approach.

This discussion brings us back to the fundamental problem: we need to know what the purpose of the model is and how it is going to be used in order to decide how to build it, and whether it’s working.

### Pneumonia Prediction

During an INFORMS competition on pneumonia predictions in hospital records — where the goal was to predict whether a patient has pneumonia — a logistic regression that included the number of diagnosis codes as a numeric feature (AUC of 0.80) didn’t do as well as the one that included it as a categorical feature (0.90). What happened?

This had to do with how the person prepared the data for the competition, as depicted in Figure 13-2.

Figure 13-2. How data preparation was done for the INFORMS competition

The diagnosis code for pneumonia was 486. So the preparer removed that (and replaced it with a “–1”) if it showed up in the record (rows are different patients; columns are different diagnoses; there is a maximum of four diagnoses; “–1” means there’s nothing for that entry).

Moreover, to avoid telling holes in the data, the preparer moved the other diagnoses to the left if necessary, so that only “–1”s were on the right.

There are two problems with this:

- If the row has only “–1”s, then you know it started out with only pneumonia.

- If the row has no “–1”s, you know there’s no pneumonia (unless there are actually five diagnoses, but that’s less common).

This alone was enough information to win the competition.

Leakage Happens

Winning a competition on leakage is easier than building good models. But even if you don’t explicitly understand and game the leakage, your model will do it for you. Either way, leakage is a huge problem with data mining contests in general.

## How to Avoid Leakage

The message here is not about how to win predictive modeling competitions. The reality is that as a data scientist, you’re at risk of producing a data leakage situation any time you prepare, clean your data, impute missing values, remove outliers, etc. You might be distorting the data in the process of preparing it to the point that you’ll build a model that works well on your “clean” dataset, but will totally suck when applied in the real-world situation where you actually want to apply it. Claudia gave us some very specific advice to avoid leakage. First, you need a strict temporal cutoff: remove all information just prior to the event of interest. For example, stuff you know before a patient is admitted. There has to be a timestamp on every entry that corresponds to the time you learned that information, not the time it occurred. Removing columns and rows from your data is asking for trouble, specifically in the form of inconsistencies that can be teased out. The best practice is to start from scratch with clean, raw data after careful consideration. Finally, you need to know how the data was created!

Claudia and her coauthors describe in the paper referenced earlier a suggested methodology for avoiding leakage as a two-stage process of tagging every observation with legitimacy tags during collection and then observing what they call a learn-predict separation.

## Evaluating Models

How do you know that your model is any good? We’ve gone through this already in some previous chapters, but it’s always good to hear this again from a master.

With powerful algorithms searching for patterns of models, there is a serious danger of overfitting. It’s a difficult concept, but the general idea is that “if you look hard enough, you’ll find something,” even if it does not generalize beyond the particular training data.

To avoid overfitting, we cross-validate and cut down on the complexity of the model to begin with. Here’s a standard picture in Figure 13-3 (although keep in mind we generally work in high dimensional space and don’t have a pretty picture to look at).

Figure 13-3. This classic image from Hastie and Tibshirani’s Elements of Statistical Learning (Springer-Verlag) shows fitting linear regression to a binary response, fitting 15-nearest neighbors, and fitting 1-nearest neighbors all on the same dataset

The picture on the left is underfit, in the middle it’s good, and on the right it’s overfit.

The model you use matters when it concerns overfitting, as shown in Figure 13-4.

Figure 13-4. The model you use matters!

Looking at Figure 13-4, unpruned decision trees are the overfitting-est (we just made that word up). This is a well-known problem with unpruned decision trees, which is why people use pruned decision trees.

### Accuracy: Meh

One of the model evaluation metrics we’ve discussed in this book is accuracy as a means to evaluate classification problems, and in particular binary classification problems. Claudia dismisses accuracy as a bad evaluation method. What’s wrong with accuracy? It’s inappropriate for regression obviously, but even for classification, if the vast majority is of binary outcomes are 1, then a stupid model can be accurate but not good (“guess it’s always 1”), and a better model might have lower accuracy.

### Probabilities Matter, Not 0s and 1s

Nobody makes decisions on the binary outcomes themselves. You want to know the probability you’ll get breast cancer; you don’t want to be told yes or no. It’s much more information to know a probability. People care about probabilities.

So how does Claudia think evaluation should be handled? She’s a proponent of evaluating the ranking and the calibration separately. To evaluate the ranking, we use the ROC curve and calculate the area under it, which typically ranges from 0.5 to 1.0. This is independent of scaling and calibration. Figure 13-5 shows an example of how to draw an ROC curve.

Figure 13-5. An example of how to draw an ROC curve

Sometimes to measure rankings, people draw the so-called lift curve shown in Figure 13-6.

Figure 13-6. The so-called lift curve

The key here is that the lift is calculated with respect to a baseline. You draw it at a given point, say 10%, by imagining that 10% of people are shown ads, and seeing how many people click versus if you randomly showed 10% of people ads. A lift of 3 means it’s 3 times better.

How do you measure calibration? Are the probabilities accurate? If the model says probability of 0.57 that I have cancer, how do I know if it’s really 0.57? We can’t measure this directly. We can only bucket those predictions and then aggregately compare those in that prediction bucket (say 0.50–0.55) to the actual results for that bucket.

For example, take a look at Figure 13-7, which shows what you get when your model is an unpruned decision tree, where the blue diamonds are buckets.

Figure 13-7. A way to measure calibration is to bucket predictions and plot predicted probability versus empirical probability for each bucket — here, we do this for an unpruned decision tree

Blue diamonds are buckets of people, say. The x-axis is the empirical, observed fraction of people in that bucket who have cancer, as an example, and the y-axis is the average predicted value for that set of people by the unpruned decision tree. This shows that decision trees don’t generally do well with calibration.

A good model would show buckets right along the x = y curve, but here we’re seeing that the predictions were much more extreme than the actual probabilities. Why does this pattern happen for decision trees?

Claudia says that this is because trees optimize purity: it seeks out pockets that have only positives or negatives. Therefore its predictions are more extreme than reality. This is generally true about decision trees: they do not generally perform well with respect to calibration.

Logistic regression looks better when you test calibration, which is typical, as shown in Figure 13-8.

Figure 13-8. Testing calibration for logistic regression

Again, blue diamonds are buckets of people. This shows that logistic regression does better with respect to calibration.

## Choosing an Algorithm

This is not a trivial question and, in particular, tests on smaller datasets may steer you wrong, because as you increase the sample size, the best algorithm might vary. Often decision trees perform very well, but only if there’s enough data.

In general, you need to choose your algorithm depending on the size and nature of your dataset, and you need to choose your evaluation method based partly on your data and partly on what you wish to be good at. Sum of squared error is the maximum likelihood loss function if your data can be assumed to be normal, but if you want to estimate the median, then use absolute errors. If you want to estimate a quantile, then minimize the weighted absolute error.

We worked on a competition about predicting the number of ratings a movie will get in the next year, and we assumed Poisson distributions. In this case, our evaluation method didn’t involve minimizing the sum of squared errors, but rather something else that we found in the literature specific to the Poisson distribution, which depends on the single parameter λ. So sometimes you need to dig around in the literature to find an evaluation metric that makes sense for your situation.

## A Final Example

Let’s put some of this together.

Say you want to raise money for a charity. If you send a letter to every person in the mailing list, you raise about $9,000. You’d like to save money and only send money to people who are likely to give — only about 5% of people generally give. How can you figure out who those people are?

If you use a (somewhat pruned, as is standard) decision tree, you get $0 profit: it never finds a leaf with majority positives.

If you use a neural network, you still make only $7,500, even if you only send a letter in the case where you expect the return to be higher than the cost.

Let’s break the problem down more. People getting the letter make two decisions: first, they decide whether or not to give, then they decide how much to give. You can model those two decisions separately, using:

E $ person =P response=`yes` person · E $ response=`yes`,person

Note that you need the first model to be well-calibrated because you really care about the number, not just the ranking. So you can try logistic regression for the first half. For the second part, you train with special examples where there are donations.

Altogether, this decomposed model makes a profit of $15,000. The decomposition made it easier for the model to pick up the signals. Note that with infinite data, all would have been good, and you wouldn’t have needed to decompose. But you work with what you got.

Moreover, you are multiplying errors with this approach, which could be a problem if you have a reason to believe that those errors are correlated.

## Parting Thoughts

According to Claudia, humans are not meant to understand data. Data is outside of our sensory systems, and there are very few people who have a near-sensory connection to numbers. We are instead designed to understand language.

We are also not meant to understand uncertainty: we have all kinds of biases that prevent this from happening that are well documented. Hence, modeling people in the future is intrinsically harder than figuring out how to label things that have already happened.

Even so, we do our best, and this is through careful data generation, meticulous consideration of what our problem is, making sure we model it with data close to how it will be used, making sure we are optimizing to what we actually desire, and doing our homework to learn which algorithms fit which tasks.

[1] http://dstillery.com

[2] https://en.wikipedia.org/wiki/Watson_(computer)

[3] https://en.wikipedia.org/wiki/Jeopardy!

[4] http://www.kdd.org/kdd-cup

[5] http://ida.felk.cvut.cz/ilp2012/

[6] http://www.kdnuggets.com/news/2008/n07/8i.html

[7] https://www.kaggle.com

[8] http://dl.acm.org/citation.cfm?id=981001
