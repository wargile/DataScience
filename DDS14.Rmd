---
title: "DDS14"
author: "Robert A. Stevens"
date: "June 5, 2016"
output: html_document
---

```{r, comment=NA}

```

*Doing Data Science*

by Rachel Schutt and Cathy O’Neil

# CHAPTER 14 Data Engineering: MapReduce, Pregel, and Hadoop

We have two contributors to this chapter, David Crawshaw and Josh Wills. Rachel worked with both of them at Google on the Google+ data science team, though the two of them never actually worked together because Josh Wills left to go to Cloudera and David Crawshaw replaced him in the role of tech lead. We can call them “data engineers,” although that term might be as problematic (or potentially overloaded) or ambiguous as “data scientist” — but suffice it to say that they’ve both worked as software engineers and dealt with massive amounts of data. If we look at the data science process from Chapter 2, Josh and David were responsible at Google for collecting data (frontend and backend logging), building the massive data pipelines to store and munge the data, and building up the engineering infrastructure to support analysis, dashboards, analytics, A/B testing, and more broadly, data science.

In this chapter we’ll hear firsthand from Google engineers about MapReduce, which was developed at Google, and then open source versions were created elsewhere. MapReduce is an algorithm and framework for dealing with massive amounts of data that has recently become popular in industry. The goal of this chapter is to clear up some of the mysteriousness surrounding MapReduce. It’s become such a buzzword, and many data scientist job openings are advertised as saying “must know Hadoop” (the open source implementation of MapReduce). We suspect these ads are written by HR departments who don’t really understand what MapReduce is good for and the fact that not all data science problems require MapReduce. But as it’s become such a data science term, we want to explain clearly what it is, and where it came from. You should know what it is, but you may not have to use it — or you might, depending on your job.

Do You Need to Know MapReduce to Be a Data Scientist?

A fun game would be to go to a conference, count how many times people say the word “MapReduce,” then ask them to actually explain it, and count how many can. We suspect not many can. Even we need to call in the experts who have spent countless hours working with it extensively. At Google, Rachel did write code using Sawzall, a programming language that had the MapReduce framework underlying its logic to process and munge the data to get it into shape for analysis and prototyping. For that matter, Cathy used an open source version of Sawzall called Pig when she worked as a data scientist at Intent Media — specifically, she used Pig in conjunction with Python in the Mortar Data framework. So we did indirectly use MapReduce, and we did understand it, but not to the extent these guys did building the underlying guts of the system.

Another reason to discuss MapReduce is that it illustrates the types of algorithms used to tackle engineering and infrastructure issues when we have lots of data. This is the third category of algorithms we brought up in Chapter 3 (the other two being machine learning algorithms and optimization algorithms). As a point of comparison, given that algorithmic thinking may be new to you, we’ll also describe another data engineering algorithm and framework, Pregel, which enables large-scale graph computation (it was also developed at Google and open sourced).

## About David Crawshaw

David Crawshaw is a Software Engineer at Google who once accidentally deleted 10 petabytes of data with a bad shell script. Luckily, he had a backup. David was trained as a mathematician, worked on Google+ in California with Rachel, and now builds infrastructure for better understanding search. He recently moved from San Francisco to New York.

David came to talk to us about MapReduce and how to deal with too much data. Before we dive in to that, let’s prime things with a thought experiment.

## Thought Experiment

How do we think about access to medical records and privacy?

On the one hand, there are very serious privacy issues when it comes to health records — we don’t want just anyone to be able to access someone’s medical history. On the other hand, certain kinds of access can save lives.

By some estimates, one or two patients died per week in a certain smallish town because of the lack of information flow between the hospital’s emergency room and the nearby mental health clinic. In other words, if the records had been easier to match, they’d have been able to save more lives. On the other hand, if it had been easy to match records, other breaches of confidence might also have occurred. Of course it’s hard to know exactly how many lives are at stake, but it’s nontrivial.

This brings up some natural questions: What is the appropriate amount of privacy in health? Who should have access to your medical records and under what circumstances?

We can assume we think privacy is a generally good thing. For example, to be an atheist is punishable by death in some places, so it’s better to be private about stuff in those conditions. But privacy takes lives too, as we see from this story of emergency room deaths.

We can look to other examples, as well. There are many egregious violations happening in law enforcement, where you have large databases of license plates, say, and people who have access can abuse that information. Arguably, though, in this case it’s a human problem, not a technical problem.

It also can be posed as a philosophical problem: to what extent are we allowed to make decisions on behalf of other people?

There’s also a question of incentives. We might cure cancer faster with more medical data, but we can’t withhold the cure from people who didn’t share their data with us.

And finally, to a given person, it might be considered a security issue. People generally don’t mind if someone has their data; they mind if the data can be used against them and/or linked to them personally.

Going back full circle to the technical issue, it’s super hard to make data truly anonymous. For example, a recent Nature study, “Unique in the Crowd: the privacy bounds of human mobility” by Yves-Alexandre de Montjoye, et al., on a dataset of 1.5 million cell-phone users in Europe showed that just four points of reference were enough to individually identify 95 percent of the people.

Recently we’ve seen people up in arms about the way the NSA collects data about its citizens (not to mention non-US citizens). In fact, as this book was close to going to print, the Edward Snowden leak occurred. The response has been a very public and loud debate about the right to privacy with respect to our government. Considering how much information is bought and sold online about individuals through information warehousers and brokers like Acxiom — which leads not just to marketing but also insurance, job, and loan information — we might want to consider having that same conversation about our right to privacy with respect to private companies as well.

## MapReduce

Here we get insight into how David, as an engineer, thinks.

He revises the question we’ve asked before in this book: what is Big Data? It’s a buzzword mostly, but it can be useful. He tried this as a working definition:

You’re dealing with Big Data when you’re working with data that doesn’t fit into your computer unit. Note that makes it an evolving definition: Big Data has been around for a long time. The IRS had taxes before computers, so by our definition, the data they had didn’t fit on their (nonexistent) computer unit.

Today, Big Data means working with data that doesn’t fit in one computer. Even so, the size of Big Data changes rapidly. Computers have experienced exponential growth for the past 40 years. We have at least 10 years of exponential growth left (and people said the same thing 10 years ago).

Given this, is Big Data going to go away? Can we ignore it?

David claims we can’t, because although the capacity of a given computer is growing exponentially, those same computers also make the data. The rate of new data is also growing exponentially. So there are actually two exponential curves, and they won’t intersect any time soon.

Let’s work through an example to show how hard this gets.

## Word Frequency Problem

Say you’re told to find the most frequent words in the following list: red, green, bird, blue, green, red, red.

The easiest approach for this problem is inspection, of course. But now consider the problem for lists containing 10,000, or 100,000, or 109 words.

The simplest approach is to list the words and then count their prevalence. Figure 14-1 shows an example code snippet from the language Go, which David loves and is helping to build (do you build a language?) and design at Google.

Figure 14-1. Example code snippet from the language Go

Because counting and sorting are fast, this scales to ~100 million words. The limit now is computer memory — if you think about it, you need to get all the words into memory twice: once when you load in the list of all words, and again when you build a way to associate a count for each word.

You can modify it slightly so it doesn’t have to have all words loaded in memory — keep them on the disk and stream them in by using a channel instead of a list. A channel is something like a stream: you read in the first 100 items, then process them, then you read in the next 100 items.

But there’s still a potential problem, because if every word is unique, and the list is super long, your program will still crash; it will still be too big for memory. On the other hand, this will probably work nearly all the time, because nearly all the time there will be repetition. Real programming is a messy game.

Hold up, computers nowadays are many-core machines; let’s use them all! Then the bandwidth will be the problem, so let’s compress the inputs, too. That helps, and moreover there are better alternatives that get complex. A heap of hashed values has a bounded size and can be well-behaved. A heap is something like a partially ordered set, and you can throw away super small elements to avoid holding everything in memory. This won’t always work, but it will in most cases.

Are You Keeping Up?

You don’t need to understand all these details, but we want you to get a flavor for the motivation for why MapReduce is even necessary.

Now we can deal with on the order of 10 trillion words, using one computer.

Now say we have 10 computers. This will get us 100 trillion words. Each computer has 1/10th of the input. Let’s get each computer to count up its share of the words. Then have each send its counts to one “controller” machine. The controller adds them up and finds the highest to solve the problem.

We can do this with hashed heaps, too, if we first learn network programming.

Now take a hundred computers. We can process a thousand trillion words. But then the “fan-in,” where the results are sent to the controller, will break everything because of bandwidth problem. We need a tree, where every group of 10 machines sends data to one local controller, and then they all send back to super controller. This will probably work.

But... can we do this with 1,000 machines? No. It won’t work. Because at that scale, one or more computer will fail. If we denote by X the variable that exhibits whether a given computer is working, so X = 0 means it works and X = 1 means it’s broken, then we can assume:

P X=0 =1−ε

But this means, when we have 1,000 computers, the chance that no computer is broken is

1−ε 1000

which is generally pretty small even if ε is small. So if ε = 0 . 001 for each individual computer, then the probability that all 1,000 computers work is 0.37, less than even odds. This isn’t sufficiently robust.

What to do?

We address this problem by talking about fault tolerance for distributed work. This usually involves replicating the input (the default is to have three copies of everything), and making the different copies available to different machines, so if one blows, another one will still have the good data. We might also embed checksums in the data, so the data itself can be audited for errors, and we will automate monitoring by a controller machine (or maybe more than one?).

In general we need to develop a system that detects errors and restarts work automatically when it detects them. To add efficiency, when some machines finish, we should use the excess capacity to rerun work, again checking for errors.

Q: Wait, I thought we were counting things?! This seems like some other awful rat’s nest we’ve gotten ourselves into.

A: It’s always like this. You cannot reason about the efficiency of fault tolerance easily; everything is complicated. And note, efficiency is just as important as correctness, because a thousand computers are worth more than your salary.

It’s like this:

• The first 10 computers are easy;

• The first 100 computers are hard; and

• The first 1,000 computers are impossible.

There’s really no hope.

Or at least there wasn’t until about eight years ago. At Google now, David uses 10,000 computers regularly.

### Enter MapReduce

In 2004 Jeff and Sanjay published their paper “MapReduce: Simplified Data Processing on Large Clusters” (and here’s another one on the underlying filesystem).

MapReduce allows us to stop thinking about fault tolerance; it is a platform that does the fault tolerance work for us. Programming 1,000 computers is now easier than programming 100. It’s a library to do fancy things.

To use MapReduce, you write two functions: a mapper function, and then a reducer function. It takes these functions and runs them on many machines that are local to your stored data. All of the fault tolerance is automatically done for you once you’ve placed the algorithm into the map/reduce framework.

The mapper takes each data point and produces an ordered pair of the form (key, value). The framework then sorts the outputs via the “shuffle,” and in particular finds all the keys that match and puts them together in a pile. Then it sends these piles to machines that process them using the reducer function. The reducer function’s outputs are of the form (key, new value), where the new value is some aggregate function of the old values.

So how do we do it for our word counting algorithm? For each word, just send it to the ordered pair with the key as that word and the value being the integer 1. So:

    red ---> ("red", 1)

    blue ---> ("blue", 1)

    red ---> ("red", 1)

Then they go into the “shuffle” (via the “fan-in”) and we get a pile of (“red”, 1)’s, which we can rewrite as (“red”, 1, 1). This gets sent to the reducer function, which just adds up all the 1’s. We end up with (“red”, 2), (“blue”, 1).

The key point is: one reducer handles all the values for a fixed key.

Got more data? Increase the number of map workers and reduce workers. In other words, do it on more computers. MapReduce flattens the complexity of working with many computers. It’s elegant, and people use it even when they shouldn’t (although, at Google it’s not so crazy to assume your data could grow by a factor of 100 overnight). Like all tools, it gets overused.

Counting was one easy function, but now it’s been split up into two functions. In general, converting an algorithm into a series of MapReduce steps is often unintuitive.

For the preceding word count, distribution needs to be uniform. If all your words are the same, they all go to one machine during the shuffle, which causes huge problems. Google has solved this using hash buckets heaps in the mappers in one MapReduce iteration. It’s called CountSketch, and it is built to handle odd datasets.

At Google there’s a real-time monitor for MapReduce jobs, a box with shards that correspond to pieces of work on a machine. It indicates through a bar chart how the various machines are doing. If all the mappers are running well, you’d see a straight line across. Usually, however, everything goes wrong in the reduce step due to nonuniformity of the data; e.g., lots of values on one key.

The data preparation and writing the output, which take place behind the scenes, take a long time, so it’s good to try to do everything in one iteration. Note we’re assuming a distributed filesystem is already there — indeed we have to use MapReduce to get data to the distributed filesystem — once we start using MapReduce, we can’t stop.

Once you get into the optimization process, you find yourself tuning MapReduce jobs to shave off nanoseconds 10−9 while processing petabytes of data. These are order shifts worthy of physicists. This optimization is almost all done in C++. It’s highly optimized code, and we try to scrape out every ounce of power we can.

## Other Examples of MapReduce

Counting words is the most basic example of MapReduce. Let’s look at another to start getting more of a feel for it. The key attribute of a problem that can be solved with MapReduce is that the data can be distributed among many computers and the algorithm can treat each of those computers separately, i.e., one computer doesn’t need to know what’s going on with any other computer.

Here’s another example where you could use MapReduce. Let’s say you had tons of timestamped event data and logs of users’ actions on a website. For each user, you might have {user_id, IP_address, zip code, ad_they_saw, did_they_click}. Suppose you wanted to count how many unique users saw ads from each zip code and how many clicked at least once.

How would you use MapReduce to handle this? You could run MapReduce keyed by zip code so that a record with a person living in zip code 90210 who clicked on an ad would get emitted to (90210,{1,1}) if that person saw an ad and clicked, or (90210,{0,1}) if they saw an ad and didn’t click.

What would this give you? At the reducer stage, this would count the total number of clicks and impressions by zip code producing output of the form (90210,{700,15530}), for example. But that’s not what you asked. You wanted the number of unique users. This would actually require two MapReduces.

First use {zip_code,user} as the key and {clicks, impressions} as the value. Then, for example ({90210,user_5321},{0,1}) or ((90210], user_5321} <- {1,1}. The reducer would emit a table that per user, per zip code, gave the counts of clicks and impressions. Your new records would now be {user, zipcode, number_clicks, number_impressions}.

Then to get the number of unique users from each zip code and how many clicked at least once, you’d need a second MapReduce job with zipcode as the key, and for each user emits {1, ifelse(clicks>0)} as the value.

So that was a second illustration of using MapReduce to count. But what about something more complicated like using MapReduce to implement a statistical model such as linear regression. Is that possible?

Turns out it is. Here’s a 2006 paper that goes through how the MapReduce framework could be used to implement a variety of machine learning algorithms. Algorithms that calculate sufficient statistics or gradients that depend upon calcuating expected values and summations can use the general approach described in this paper, because these calculations may be batched, and are expressible as a sum over data points.

### What Can’t MapReduce Do?

Sometimes to understand what something is, it can help to understand what it isn’t. So what can’t MapReduce do? Well, we can think of lots of things, like give us a massage. But you’d be forgiven for thinking MapReduce can solve any data problem that comes your way.

But MapReduce isn’t ideal for, say, iterative algorithms where you take a just-computed estimation and use it as input to the next stage in the computation — this is common in various machine learning algorithms that use steepest descent convergence methods. If you wanted to use MapReduce, it is of course possible, but it requires firing up many stages of the engine. Other newer approaches such as Spark might be better suited, which in this context means more efficient.

## Pregel

Just as a point of contrast, another algorithm for processing large-scale data was developed at Google called Pregel. This is a graph-based computational algorithm, where you can imagine the data itself has a graph-based or network-based structure. The computational algorithm allows nodes to communicate with other nodes that they are connected to. There are also aggregators that are nodes that have access to the information that all the nodes have, and can, for example, sum together or average any information that all the nodes send to them.

The basis of the algorithm is a series of supersteps that alternate between nodes sending information to their neighbors and nodes sending information to the aggregators. The original paper is online if you want to read more. There’s also an open source version of Pregel called Giraph.

## About Josh Wills

Josh Wills is Cloudera’s director of data science, working with customers and engineers to develop Hadoop-based solutions across a wide range of industries. More on Cloudera and Hadoop to come. Prior to joining Cloudera, Josh worked at Google, where he worked on the ad auction system and then led the development of the analytics infrastructure used in Google+. He earned his bachelor’s degree in mathematics from Duke University and his master’s in operations research from The University of Texas at Austin.

Josh is also known for pithy data science quotes, such as: “I turn data into awesome” and the one we saw way back in the start of the book: “data scientist (noun): Person who is better at statistics than any software engineer and better at software engineering than any statistician.” Also this gem: “I am Forrest Gump, I have a toothbrush, I have a lot of data and I scrub.”

Josh primed his topic with a thought experiment first.

## Thought Experiment

How would you build a human-powered airplane? What would you do? How would you form a team?

Maybe you’d run an X prize competition. This is exactly what some people did, for $50,000, in 1950. It took 10 years for someone to win it. The story of the winner is useful because it illustrates that sometimes you are solving the wrong problem.

Namely, the first few teams spent years planning, and then their planes crashed within seconds. The winning team changed the question to: how do you build an airplane you can put back together in four hours after a crash? After quickly iterating through multiple prototypes, they solved this problem in six months.

## On Being a Data Scientist

Josh had some observations about the job of a data scientist. A data scientist spends all their time doing data cleaning and preparation — a full 90% of the work is this kind of data engineering. When deciding between solving problems and finding insights, a data scientist solves problems. A bit more on that: start with a problem, and make sure you have something to optimize against. Parallelize everything you do.

It’s good to be smart, but being able to learn fast is even better: run experiments quickly to learn quickly.

### Data Abundance Versus Data Scarcity

Most people think in terms of scarcity. They are trying to be conservative, so they throw stuff away. Josh keeps everything. He’s a fan of reproducible research, so he wants to be able to rerun any phase of his analysis. He keeps everything. This is great for two reasons. First, when he makes a mistake, he doesn’t have to restart everything. Second, when he gets new sources of data, it’s easy to integrate them in the point of the flow where it makes sense.

### Designing Models

Models always turn into crazy Rube Goldberg machines, a hodge-podge of different models. That’s not necessarily a bad thing, because if they work, they work. Even if you start with a simple model, you eventually add a hack to compensate for something. This happens over and over again; it’s the nature of designing the model.

Mind the gap

The thing you’re optimizing with your model isn’t the same as the thing you’re optimizing for your business.

Example: friend recommendations on Facebook don’t optimize you accepting friends, but rather maximizing the time you spend on Facebook. Look closely: the suggestions are surprisingly highly populated by attractive people of the opposite sex.

How does this apply in other contexts? In medicine, they study the effectiveness of a drug instead of the health of the patients. They typically focus on success of surgery rather than well-being of the patient.

## Economic Interlude: Hadoop

Let’s go back to MapReduce and Hadoop for a minute. When Josh graduated in 2001, there were two options for file storage — databases and filers — which Josh compared on four dimensions: schema, processing, reliability, and cost (shown in Table 14-1).

Table 14-1. Options for file storage back in 2001

Databases Schema Structured

Processing Intensive processing done where data is stored Reliability Somewhat reliable

Cost Expensive at scale

Filers

No schemas

No data processing capability Reliable

Expensive at scale

Since then we’ve started generating lots more data, mostly from the Web. It brings up the natural idea of a data economic indicator: return on byte. How much value can we extract from a byte of data? How much does it cost to store? If we take the ratio, we want it to be bigger than one, or else we discard the data.

Of course, this isn’t the whole story. There’s also a Big Data economic law, which states that no individual record is particularly valuable, but having every record is incredibly valuable. So, for example, for a web index, recommendation system, sensor data, or online advertising, one has an enormous advantage if one has all the existing data, even though each data point on its own isn’t worth much.

### A Brief Introduction to Hadoop

Back before Google had money, it had crappy hardware. So to handle all this data, it came up with idea of copying data to multiple servers. It did this physically at the time, but then automated it. The formal automation of this process was the genesis of GFS.

There are two core components to Hadoop, which is the open source version of Google’s GFS and MapReduce. (You can read more about the origin stories of Hadoop elsewhere. We’ll give you a hint. A small open source project called Nutch and Yahoo! were involved.) The first component is the distributed filesystem (HDFS), which is based on the Google filesystem. The data is stored in large files, with block sizes of 64 MB to 256 MB. The blocks are replicated to multiple nodes in the cluster. The master node notices if a node dies. The second component is MapReduce, which David Crawshaw just told us all about.

Also, Hadoop is written in Java, whereas Google stuff is in C++. Writing MapReduce in the Java API not pleasant. Sometimes you have to write lots and lots of MapReduces. However, if you use Hadoop streaming, you can write in Python, R, or other high-level languages. It’s easy and convenient for parallelized jobs.

### Cloudera

Cloudera was cofounded by Doug Cutting, one of the creators of Hadoop, and Jeff Hammerbacher, who we mentioned back in Chapter 1 because he co-coined the job title “data scientist” when he worked at Facebook and built the data science team there.

Cloudera is like Red Hat for Hadoop, by which we mean they took an open source project and built a company around it. It’s done under the aegis of the Apache Software Foundation. The code is available for free, but Cloudera packages it together, gives away various distributions for free, and waits for people to pay for support and to keep it up and running.

Apache Hive is a data warehousing system on top of Hadoop. It uses a SQL-based query language (includes some MapReduce-specific extensions), and it implements common join and aggregation patterns. This is nice for people who know databases well and are familiar with stuff like this.

## Back to Josh: Workflow

With respect to how Josh would approach building a pipeline using MapReduce, he thinks of his basic unit of analysis as a record. We’ve repeatedly mentioned “timestamped event data,” so you could think of a single one of those as a record, or you could think of transaction records that we discussed in fraud detection or credit card transactions. A typical workflow would then be something like:

1. Use Hive (a SQL-like language that runs on Hadoop) to build records that contain everything you know about an entity (say a person) (intensive MapReduce stuff).

2. Write Python scripts to process the records over and over again (faster and iterative, also MapReduce).

3. Update the records when new data arrives.

Note the scripts in phase 2 are typically map-only jobs, which makes parallelization easy.

Josh prefers standard data formats: text is big and takes up space. Thrift, Avro, and protocol buffers are more compact, binary formats. He also encourages you to use the code and metadata repository Github. He doesn’t keep large data files in Git.

## So How to Get Started with Hadoop?

If you are working at a company that has a Hadoop cluster, it’s likely that your first experience will be with Apache Hive, which provides a SQL-style abstraction on top of HDFS and MapReduce. Your first MapReduce job will probably involve analyzing logs of user behavior in order to get a better understanding of how customers are using your company’s products.

If you are exploring Hadoop and MapReduce on your own for building analytical applications, there are a couple of good places to start. One option is to build a recommendation engine using Apache Mahout, a collection of machine learning libraries and command-line tools that works with Hadoop. Mahout has a collaborative filtering engine called Taste that can be used to create recommendations given a CSV file of user IDs, item IDs, and an optional weight that indicates the strength of the relationship between the user and the item. Taste uses the same recommendation algorithms that Netflix and Amazon use for building recommendations for their users.

Every algorithm is editorial.
